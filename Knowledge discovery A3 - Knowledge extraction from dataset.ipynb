{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fe1b785-92fb-48dd-9ecb-e3d9d5c582a5",
   "metadata": {},
   "source": [
    "# Knowledge Discovery and Pattern Extraction - A3 - Mat. 257832\n",
    "The objective of the following notebook is to try various strategies to extract knowledge from the \"Openreview\" dataset (\"https://github.com/Seafoodair/Openreview\") using a combination of traditional ML plus the usage of LLMs. In particular, we focus on the \"2020\" dataset and we try to extract knowledge from the papers' reviews in order to extract possible ways in which authors can improve their own papers. We will mainly showcase two strategies: in the first version, we will use some basic NLP techniques to preprocess the dataset and obtain a list of n-grams to give as input to the LLM in order to infer useful information for our purpose. In the second version, we will divide the dataset in sentences and represent them as word embeddings. Then, we are going to cluster these representations to extract topics using specific libraries (in particular, SBERT (https://github.com/UKPLab/sentence-transformers) and BERTopic (https://github.com/MaartenGr/BERTopic)). We use this second approach to show a more complex possible solution to the problem at hand and also to test whether topic based models can be used to infer knowledge nuggets given a more limited context. In both these cases we will use a cleaned up version of the datatset: this is both to reduce the dataset's size, and thus the complexity and the duration of the various steps in both strategies, and also because we have reasons to believe, after inspecting the original dataset, that it contains a set of words that have no semantic meaning (words like \"a, an, the, or, is\" and so on) or are not useful to infer possible knowledge (ex: the summary of the paper's contents) that we can safely disregard, while not losing information in the process. This is the reason why we won't showcase a method using the entire original dataset. In this notebook, we will also try to consider the possible number of LLM tokens that we would use for each specific approach. Also, we are going to ignore the naive way of approaching this exercise, which would be to delegate all the tasks to the LLM, for two reasons: <br>\n",
    "    1. Budget: if the dataset is big enough, this would involve the usage of a lot of tokens, requiring a not indifferent budget to be allocated for the task <br>\n",
    "    2. Hallucinations: LLMs work better on a limited subset of data: if ours is too big, it would lead to a degradation in performance and to worse overall results, leading to an unacceptable waste of resources.\n",
    "\n",
    "# First version - Basic NLP techniques\n",
    "## Cleaning up the datatset\n",
    "The first thing that we need to do is understand how to clean the dataset in a way that still maintains the relevant information we care about. We, of course, want to automate this process, as doing it manually would render our task redundant, due to the fact that we would be able to infer the knowledge we are seeking by ourselves. The first thing we can do is eliminate those words that offer very little semantic value by using a stopword list, so to reduce the dataset's size, while also keeping intact the core concepts exposed in the reviews. For this purpose, we will use NLTK's stop words for the English language updated with a list of words and punctuations that have very little semantic content but that frequently appear throughout the dataset. Note this process could be fine-tuned to obtain a better version of the cleared datatset we are going to obtain, which is something, however, we are not going to do in this notebook for simplicity's sake.\n",
    "\n",
    "## Reducing the dataset's size while maintaining context: Bigram's approach\n",
    "After this, we need to ask ourselves if we want to keep the relationship existing between a review and a paper: in this case, we have decided not to keep it, due to our objective being the one of finding patterns in reviews that can tell us what aspects an author needs to pay attention while writing a paper, not to find ways to improve that specific paper. Thus, we may want to transform the review set into a frequency list, trying to see what are the most frequently used words, while trying to put a threshold to it so to make more technical or rare words not appear in the final list. However, by completely disregarding the context, we may lose various degree of information in the process, which may either make the LLM spout out non-sensical data or, in any case, reduce drastically the quality of the possible output. Inversly, this would decrease significantly the input to the LLM, making us use way less tokens than if we tried to feed it even the cleaned up review list. A good compromise is to use n-grams to try and keep a limited context through which to try and keep some amount of information that would have been lost by simply using a frequency list. Because of this, we have decided to compute the bigrams of every single review and then to calculate the frequency of the same, while also applying a threshold to try and reduce the size of the input to pass to the LLM, so to discard the less useful information - where we can assume this is the case due to the low frequency that makes it so that it does not pass the threshold. We then save the output onto a file and give it as input to the chosen LLM - in this case, ChatGPT (we don't use their APIs due to not having an API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2894f5c-83fa-4c4b-b807-9d76febbf576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/nyasatte/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wget in /home/nyasatte/.local/lib/python3.10/site-packages (3.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /home/nyasatte/.local/lib/python3.10/site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /home/nyasatte/.local/lib/python3.10/site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/nyasatte/.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /home/nyasatte/.local/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/nyasatte/.local/lib/python3.10/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /home/nyasatte/.local/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/nyasatte/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nyasatte/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-05 15:06:01--  https://github.com/Seafoodair/Openreview/raw/refs/heads/master/data/ICLR%20data/tp_2020conference.xlsx\n",
      "Risoluzione di github.com (github.com)... 140.82.121.3\n",
      "Connessione a github.com (github.com)|140.82.121.3|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 302 Found\n",
      "Posizione: https://raw.githubusercontent.com/Seafoodair/Openreview/refs/heads/master/data/ICLR%20data/tp_2020conference.xlsx [segue]\n",
      "--2025-05-05 15:06:01--  https://raw.githubusercontent.com/Seafoodair/Openreview/refs/heads/master/data/ICLR%20data/tp_2020conference.xlsx\n",
      "Risoluzione di raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connessione a raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 10997810 (10M) [application/octet-stream]\n",
      "Salvataggio in: ‘tp_2020conference.xlsx.3’\n",
      "\n",
      "tp_2020conference.x 100%[===================>]  10,49M  3,39MB/s    in 3,1s    \n",
      "\n",
      "2025-05-05 15:06:05 (3,39 MB/s) - ‘tp_2020conference.xlsx.3’ salvato [10997810/10997810]\n",
      "\n",
      "Installed necessary dependencies!\n"
     ]
    }
   ],
   "source": [
    "#install the required libraries\n",
    "%pip install pandas\n",
    "%pip install wget\n",
    "%pip install openpyxl\n",
    "%pip install nltk\n",
    "\n",
    "import pandas as pd\n",
    "import wget\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#get the dataset\n",
    "!wget https://github.com/Seafoodair/Openreview/raw/refs/heads/master/data/ICLR%20data/tp_2020conference.xlsx\n",
    "\n",
    "print(\"Installed necessary dependencies!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8bafb30-1c01-4e46-ad6c-a15d0c69fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams ready!\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "\n",
    "#load the dataset and extract the review column\n",
    "df = pd.read_excel(\"tp_2020conference.xlsx\", sheet_name='tp_2020conference', usecols=\"R\", engine='openpyxl')\n",
    "#transform the Dataframe into a list\n",
    "reviews = df[\"review\"].tolist()\n",
    "#tokenize it and lower it\n",
    "tokenized_reviews = [word_tokenize(i.lower()) for i in reviews]\n",
    "#print(tokenized_reviews[1])\n",
    "#declare the stop words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#add to the stop_words the refuses introduced while reading the file. We also include punctuation and words like \"et al\" that are used only for referencing to other papers\n",
    "stop_words.update([\"review\", \"#\", \".\", \":\", \";\", \",\", \"-\", \"?\", \"!\", \"(\", \")\", \"[\", \"]\", \"'\", \"=\", \"*\", \"+\", \"%\", \"/\", \"{\", \"}\", \"--\", \"...\", \"&\", \"et\", \"al\"])\n",
    "#filter the reviews, throwing away the words included in the stop words list\n",
    "filtered_reviews = [[word for word in review if word not in stop_words] for review in tokenized_reviews]\n",
    "#create bigrams to keep a bit of context\n",
    "bgs = [list(nltk.bigrams(sublist)) for sublist in filtered_reviews]\n",
    "#now we flatten the bigrams' list to compute the frequency\n",
    "all_bigrams = [bg for sublist in bgs for bg in sublist]\n",
    "#we compute the frequency of each bigram\n",
    "fdist = nltk.FreqDist(all_bigrams)\n",
    "#we set a threshold and throw away all the bigrams that appear less than it\n",
    "threshold = 4 \n",
    "filtered_bigrams = [(k, v) for k, v in fdist.items() if v >= threshold]\n",
    "#we save the bigrams onto a file\n",
    "with open('out.txt', 'w') as output:\n",
    "    output.write('\\n'.join('%s %s' % x for x in filtered_bigrams))\n",
    "    output.close()\n",
    "print(\"Bigrams ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e65632-a0d1-41c3-96e2-bbffa06b1e1b",
   "metadata": {},
   "source": [
    "After generating the input file, we gave it to ChatGPT with the following prompt:\n",
    "**\"The following text file contains bigrams associated with their relative frequency. The following bigrams come from a series of reviews about scientific papers. Your task is to extrapolate information regarding how could an author write better papers using *only* the data provided in the text file.\"** <br>\n",
    "The following is the answer that ChatGPT provided, given the text file generated above:\n",
    "***\n",
    "Based on the bigram frequency data from the reviews, we can extract several practical suggestions for authors aiming to write better scientific papers:\n",
    "\n",
    "✅ 1. Clearly Present the Main Contribution\n",
    "\n",
    "    Bigrams: ('main', 'contribution'), ('contribution', 'paper'), ('paper', 'presents'), ('paper', 'focuses')\n",
    "\n",
    "    Insight: Reviewers value clarity in the main idea or novelty of the paper. Authors should explicitly state what their core contribution is and why it matters.\n",
    "\n",
    "✅ 2. Write Clearly and Structure Well\n",
    "\n",
    "    Bigrams: ('well', 'written'), ('well', 'structured'), ('easy', 'follow'), ('clear', 'explanation')\n",
    "\n",
    "    Insight: Good papers are easy to read and have a logical flow. Spend time on writing style, clarity, and document organization.\n",
    "\n",
    "✅ 3. Provide Strong Experimental Evidence\n",
    "\n",
    "    Bigrams: ('empirical', 'study'), ('ablation', 'study'), ('convincing', 'experiments'), ('experiments', 'show'), ('experimental', 'results')\n",
    "\n",
    "    Insight: Papers that support claims with thorough and diverse experiments are more convincing. Ablation studies, baselines, and robust empirical comparisons are expected.\n",
    "\n",
    "✅ 4. Discuss Limitations and Scope Honestly\n",
    "\n",
    "    Bigrams: ('limited', 'novelty'), ('main', 'concern'), ('concern', 'paper'), ('would', 'helpful')\n",
    "\n",
    "    Insight: Acknowledging the limitations of your approach demonstrates scientific maturity and often leads to constructive feedback rather than rejection.\n",
    "\n",
    "✅ 5. Ensure Theoretical Soundness\n",
    "\n",
    "    Bigrams: ('theoretical', 'analysis'), ('theoretical', 'results'), ('provide', 'theoretical')\n",
    "\n",
    "    Insight: Especially in ML and related fields, solid theoretical backing adds significant weight. Even empirical papers benefit from theoretical justification.\n",
    "\n",
    "✅ 6. Compare Against Strong Baselines\n",
    "\n",
    "    Bigrams: ('compare', 'baselines'), ('baselines', 'used'), ('better', 'performance')\n",
    "\n",
    "    Insight: Authors should benchmark their methods against well-known baselines, not just toy examples or weak comparisons.\n",
    "\n",
    "✅ 7. Add Novelty Beyond Simple Extensions\n",
    "\n",
    "    Bigrams: ('bit', 'incremental'), ('limited', 'results'), ('novelty', 'significance')\n",
    "\n",
    "    Insight: Reviewers are wary of papers that only make incremental progress or repackage existing ideas. Ensure meaningful innovation.\n",
    "\n",
    "✅ 8. Highlight Practical Impact or Applications\n",
    "\n",
    "    Bigrams: ('improve', 'performance'), ('useful', 'insights'), ('important', 'questions')\n",
    "\n",
    "    Insight: Demonstrate how the work could be applied in practice or how it advances the field in understanding or usability.\n",
    "\n",
    "✅ 9. Use Proper Citations and Related Work\n",
    "\n",
    "    Bigrams: ('related', 'work'), ('prior', 'work'), ('cited', 'paper')\n",
    "\n",
    "    Insight: A solid literature review shows awareness of the field and can position the new work more effectively.\n",
    "\n",
    "✅ 10. Respond to Reviewer Feedback Thoughtfully\n",
    "\n",
    "    Bigrams: ('authors', 'response'), ('thank', 'authors'), ('authors', 'address')\n",
    "\n",
    "    Insight: Papers often undergo multiple revisions, and good author responses can sway decisions positively. Be clear, concise, and professional in rebuttals.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add7295f-9e44-43ab-b26a-666b13e60f4b",
   "metadata": {},
   "source": [
    "While the above response seems acceptable enough, if we observe the bigrams which were used to come to the cited conclusion, it seems like some of them are not particularly relevant to the answer provided by the LLM. This could mean that the model either lied to us - effectively creating answers that may not actually be present in the reviews - or that it tried to find the closest thing it could in order to justify its response, basically clutching at straws to justify its conclusion. Because of this, we tried to create a new chat and gave it the same input file and the same prompt. This time, the results were a bit different from before:\n",
    "***\n",
    "Based on the frequency of bigrams in your text file—which represents common phrases found in reviews of scientific papers—we can extract several key patterns and suggestions that could help authors write better papers. Here's a breakdown of recurring themes and how they translate into actionable advice: <br>\n",
    "🔍 1. Clearly Present the Main Contribution\n",
    "\n",
    "    Bigrams: ('main', 'contribution'), ('contribution', 'paper'), ('summary', 'paper')\n",
    "\n",
    "    Advice: Ensure that the primary contribution of your work is explicit and emphasized early in the paper, typically in the abstract and introduction.\n",
    "\n",
    "🧪 2. Back Claims with Rigorous Experiments\n",
    "\n",
    "    Bigrams: ('ablation', 'study'), ('empirical', 'evidence'), ('experiments', 'show'), ('lack', 'rigorous')\n",
    "\n",
    "    Advice: Support your claims with strong experimental evidence, including ablation studies and comparative results with baselines.\n",
    "\n",
    "🧠 3. Motivate and Explain Your Method Well\n",
    "\n",
    "    Bigrams: ('well', 'motivated'), ('intuition', 'behind'), ('method', 'described'), ('described', 'well')\n",
    "\n",
    "    Advice: Provide a clear motivation and intuitive explanation for your proposed method, not just mathematical formalism.\n",
    "\n",
    "✍️ 4. Write Clearly and Structure Well\n",
    "\n",
    "    Bigrams: ('well', 'written'), ('well', 'structured'), ('overall', 'paper'), ('easy', 'follow')\n",
    "\n",
    "    Advice: Use clear language and structure the paper logically (e.g., Introduction → Related Work → Methods → Results → Conclusion).\n",
    "\n",
    "📊 5. Include Strong Comparisons and Baselines\n",
    "\n",
    "    Bigrams: ('compare', 'baselines'), ('fair', 'comparison'), ('baseline', 'methods')\n",
    "\n",
    "    Advice: Always include well-established baselines and discuss how your method compares to them quantitatively and qualitatively.\n",
    "\n",
    "🔧 6. Perform and Report Hyperparameter Tuning\n",
    "\n",
    "    Bigrams: ('hyperparameter', 'tuning'), ('parameters', 'distribution')\n",
    "\n",
    "    Advice: Explicitly describe the tuning process to demonstrate the robustness and fairness of your experiments.\n",
    "\n",
    "📉 7. Address Limitations Transparently\n",
    "\n",
    "    Bigrams: ('method', 'limited'), ('pointing', 'limitations')\n",
    "\n",
    "    Advice: Acknowledge the limitations of your method honestly; this can build credibility and guide future work.\n",
    "\n",
    "📈 8. Demonstrate Theoretical and Practical Value\n",
    "\n",
    "    Bigrams: ('theoretical', 'analysis'), ('empirical', 'study'), ('improve', 'performance')\n",
    "\n",
    "    Advice: Complement empirical work with theoretical insights if possible. Discuss real-world relevance.\n",
    "\n",
    "🧾 9. Improve Novelty and Avoid Incrementality\n",
    "\n",
    "    Bigrams: ('limited', 'novelty'), ('paper', 'incremental')\n",
    "\n",
    "    Advice: Aim for meaningful innovation, not minor tweaks on existing methods. Clearly explain what’s new and why it matters.\n",
    "\n",
    "🤝 10. Respond Thoroughly to Review Feedback\n",
    "\n",
    "    Bigrams: ('thank', 'authors'), ('authors', 'response'), ('detailed', 'comments')\n",
    "\n",
    "    Advice: In the rebuttal phase, give thoughtful and specific responses to reviewers’ comments, ideally with updated results or clarifications.\n",
    "***\n",
    "\n",
    "There are some notable differences between the two outputs (point 6 is an example of this), but they also share various points that are just worded a bit differently. When asked about how it reached these results, here is what ChatGPT answered: <br>\n",
    "\n",
    "***\n",
    "✅ Methodology:\n",
    "\n",
    "The file contains bigrams (pairs of consecutive words) along with their frequency counts—this shows how often each phrase appears in a collection of reviews about scientific papers. By analyzing which bigrams occur most frequently, especially those that contain meaningful feedback or evaluative language, we can infer patterns that reflect common expectations, critiques, and praise from reviewers. <br>\n",
    "🧠 Key Steps Taken:\n",
    "\n",
    "    Identified High-Frequency Bigrams:\n",
    "    I focused on commonly repeated bigrams like:\n",
    "\n",
    "        ('main', 'contribution') – ~400+ mentions\n",
    "\n",
    "        ('ablation', 'study') and ('empirical', 'evidence') – frequent in experimental validations\n",
    "\n",
    "        ('well', 'written'), ('well', 'structured'), and ('easy', 'follow') – indicators of writing quality\n",
    "\n",
    "    Mapped Phrases to Themes:\n",
    "    For instance:\n",
    "\n",
    "        Phrases like ('method', 'limited'), ('limited', 'novelty'), ('paper', 'incremental') reflect negative feedback about novelty, hence a recommendation to aim for stronger contributions.\n",
    "\n",
    "        ('compare', 'baselines'), ('fair', 'comparison'), and ('baseline', 'methods') emphasize the need to benchmark fairly.\n",
    "\n",
    "    Inferred Reviewer Expectations:\n",
    "\n",
    "        Phrases like ('intuition', 'behind'), ('method', 'described'), and ('well', 'motivated') suggest reviewers want well-explained methodologies.\n",
    "\n",
    "        Phrases like ('results', 'convincing') and ('empirical', 'study') show the value placed on thorough experiments.\n",
    "\n",
    "    Excluded Outliers or Rare Mentions:\n",
    "\n",
    "        Low-frequency bigrams like ('recency', 'bias') or ('visual', 'system') weren't used unless they appeared in clear thematic clusters.\n",
    "\n",
    "    Synthesized Advice:\n",
    "    I grouped bigrams into thematic clusters (e.g., writing quality, experimental rigor, novelty) and translated those into actionable recommendations for authors.\n",
    "***\n",
    "\n",
    "The above explanation makes sense and seems reasonable enough, however it leaves a bit to be desired: in particular, the \"Excluded Outliers\" section reveals that certain outliers were left out, with one of the examples being \"recency bias\", which could have been a valid constructive criticism to give to an possible author. When asked about what were the outliers, however, ChatGPT provided a satisfying response to the above problematic: <br>\n",
    "\n",
    "***\n",
    "🚫 Outliers in the Bigram Data\n",
    "\n",
    "Outliers here refer to bigrams that appear very infrequently (typically ≤ 4–5 times) and do not contribute meaningfully to common reviewer expectations or themes about paper quality. These often:\n",
    "\n",
    "    Reflect niche topics,\n",
    "\n",
    "    Seem context-specific or dataset-specific,\n",
    "\n",
    "    Don’t generalize into actionable feedback for most authors.\n",
    "\n",
    "⚠️ Examples of Outlier Bigrams (Low Frequency, Low General Relevance):\n",
    "\n",
    "    Highly Specific Terms or Datasets\n",
    "\n",
    "        ('mnist', 'cifar-10') — Appeared ~60 times, but specific to certain benchmarks, not general guidance.\n",
    "\n",
    "        ('visual', 'system'), ('super', 'mario'), ('gini', ...) — Too narrow or domain-specific.\n",
    "\n",
    "    Low Frequency and Context-Specific\n",
    "\n",
    "        ('recency', 'bias') — Appears 5 times, possibly tied to a specific criticism but not general advice.\n",
    "\n",
    "        ('concerns', 'key'), ('key', 'concern') — Used only a handful of times and not elaborated.\n",
    "\n",
    "        ('subset', 'imagenet'), ('imagenet', 'since') — Refers to a dataset usage detail.\n",
    "\n",
    "    Ambiguous or Fragmented Phrases\n",
    "\n",
    "        ('still', 'true'), ('even', 'model'), ('words', 'would') — These are not meaningful without broader sentence context.\n",
    "\n",
    "        ('symmetric', 'cross'), ('importance', 'features') — Might be valid within a technical discussion, but not frequent or general enough.\n",
    "\n",
    "    Quirky, Non-Actionable Mentions\n",
    "\n",
    "        ('human', 'eye'), ('human', 'visual') — Interesting but do not provide general guidance on writing better papers.\n",
    "\n",
    "        ('f', 'x'), ('<', '1') — Out-of-context mathematical expressions.\n",
    "\n",
    "🧹 Why These Were Excluded from Recommendations\n",
    "\n",
    "They are:\n",
    "\n",
    "    Too narrow to apply across most papers.\n",
    "\n",
    "    Lack generalizable reviewer feedback.\n",
    "\n",
    "    Not aligned with themes like novelty, clarity, rigor, or experimental support.\n",
    "***\n",
    "\n",
    "Some of the above responses, however, can make us understand that maybe the context of the bigram is a bit too limited, as even ChatGPT highlights itself.\n",
    "\n",
    "## Trying to increase the context: Trigram's approach\n",
    "In order to see whether the results were going to be different given an ampler context, we have decided to try and follow the same approach as above, but this time using trigrams. Thus, we generated a text file containing the trigrams by using the same methodology as previously explained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ded234-35b3-4500-bf24-9a7491b0f7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import trigrams\n",
    "\n",
    "#create trigrams to keep a bit of context. Note that we have already filtered the reviews, it's unnecessary to perform the cleaning again\n",
    "trigrams = [list(nltk.trigrams(sublist)) for sublist in filtered_reviews]\n",
    "#now we flatten the trigrams' list to compute the frequency\n",
    "all_trigrams = [tg for sublist in trigrams for tg in sublist]\n",
    "#we compute the frequency of each trigram\n",
    "fdist = nltk.FreqDist(all_trigrams)\n",
    "#we set a threshold and throw away all the trigrams that appear less than it\n",
    "threshold = 4 \n",
    "filtered_trigrams = [(k, v) for k, v in fdist.items() if v >= threshold]\n",
    "#we write the trigrams onto a file\n",
    "with open('out_tri.txt', 'w') as output:\n",
    "    output.write('\\n'.join('%s %s' % x for x in filtered_trigrams))\n",
    "    output.close()\n",
    "print(\"Trigrams ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d18af-bccd-4150-9bd0-95e204f2badd",
   "metadata": {},
   "source": [
    "After generating the input file, we gave it to ChatGPT with the following prompt:\n",
    "**\"The following text file contains trigrams associated with their relative frequency. The following trigrams come from a series of reviews about scientific papers. Your task is to extrapolate information regarding how could an author write better papers using only the data provided in the text file.\"** <br>\n",
    "The following is the answer that ChatGPT provided, given the text file generated above:\n",
    "***\n",
    "\n",
    "Based on the trigrams in your file—extracted from reviews of scientific papers—we can derive several practical insights that authors can use to write better papers. Here's a breakdown of key takeaways and suggestions: <br>\n",
    "📌 1. Ensure Your Paper Is Well-Written and Organized\n",
    "\n",
    "    Frequent trigrams:\n",
    "\n",
    "        ('paper', 'well', 'written') – 581 times\n",
    "\n",
    "        ('overall', 'paper', 'well') – 107\n",
    "\n",
    "        ('paper', 'well-organized', 'easy'), ('well-organized', 'easy', 'read')\n",
    "\n",
    "Advice: Clarity and structure are essential. Reviewers consistently praise papers that are easy to read and logically structured. <br>\n",
    "📌 2. Clearly Communicate the Main Contribution\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('main', 'contribution', 'paper') – 182\n",
    "\n",
    "        ('major', 'contribution', 'paper'), ('technical', 'contribution', 'paper')\n",
    "\n",
    "Advice: Highlight your paper’s main contributions early (e.g., in the introduction) and clearly. <br>\n",
    "📌 3. Make Sure the Proposed Method Is Well-Motivated\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('method', 'well', 'motivated')\n",
    "\n",
    "        ('approach', 'well', 'motivated')\n",
    "\n",
    "Advice: Justify the need for your method or framework. Connect it to existing challenges or limitations in the field. <br>\n",
    "📌 4. Provide Strong and Clear Experimental Results\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('experimental', 'results', 'convincing') – 48\n",
    "\n",
    "        ('experiments', 'show', 'proposed'), ('results', 'show', 'proposed')\n",
    "\n",
    "Advice: Back up your claims with well-executed experiments. Include ablation studies, baselines, and performance comparisons. <br>\n",
    "📌 5. Address Novelty Clearly\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('method', 'limited', 'novelty'), ('novelty', 'proposed', 'method')\n",
    "\n",
    "        Negative: ('concern', 'paper', 'lack'), ('contribution', 'paper', 'incremental')\n",
    "\n",
    "Advice: Explicitly explain what’s new and how your work differs from prior work. <br>\n",
    "📌 6. Improve Explanations and Visualizations\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('could', 'authors', 'add'), ('authors', 'add', 'explanation')\n",
    "\n",
    "        ('would', 'nice', 'see') – 92\n",
    "\n",
    "        ('figure', '3', 'looks'), ('show', 'qualitative', 'results')\n",
    "\n",
    "Advice: Add more illustrative figures, qualitative results, or clearer explanations where needed. <br>\n",
    "📌 7. Include a Strong Related Work Section\n",
    "\n",
    "    Trigram:\n",
    "\n",
    "        ('related', 'work', 'section') – 223\n",
    "\n",
    "        Critique: ('related', 'works', 'missing')\n",
    "\n",
    "Advice: Thoroughly cover relevant literature and compare your method directly with related approaches. <br>\n",
    "📌 8. Respond Thoughtfully to Reviewer Feedback\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('thank', 'authors', 'response') – 44\n",
    "\n",
    "        ('rebuttal', 'thank', 'authors')\n",
    "\n",
    "Advice: Be professional and comprehensive when responding to reviewers, especially in rebuttals. <br>\n",
    "📌 9. Balance Theory and Empirical Work\n",
    "\n",
    "    Trigrams:\n",
    "\n",
    "        ('theoretical', 'analysis', 'proposed'), ('empirical', 'results', 'convincing')\n",
    "\n",
    "Advice: If your paper is theoretical, show practical relevance. If empirical, include some theoretical justification when possible. <br>\n",
    "📌 10. Avoid Common Pitfalls\n",
    "\n",
    "    Negative trigrams:\n",
    "\n",
    "        ('paper', 'poorly', 'written'), ('method', 'limited', 'novelty'), ('lean', 'towards', 'rejection')\n",
    "\n",
    "Advice: Avoid unclear writing, insufficient novelty, and weak experimental backing.\n",
    "***\n",
    "\n",
    "The above results do not differ that much from the ones given by using the bigram approach, but this time the bigger context makes it easier to understand the reasons why ChatGPT could come up with the answer it gave and makes it feel more convincing and less like an hallucination. When asked how it arrived at these results, ChatGPT gave the same answer as for the bigrams case: <br>\n",
    "\n",
    "***\n",
    "🔍 Step 1: Analyzing High-Frequency Trigrams\n",
    "\n",
    "I looked through the trigrams and focused on those with high frequencies — these indicate recurring themes in peer review comments. Trigrams like:\n",
    "\n",
    "    ('paper', 'well', 'written') (581)\n",
    "\n",
    "    ('related', 'work', 'section') (223)\n",
    "\n",
    "    ('main', 'contribution', 'paper') (182)\n",
    "\n",
    "    ('experimental', 'results', 'convincing') (48)\n",
    "\n",
    "These tell us what reviewers commonly praise or discuss, so I grouped them into categories based on context.\n",
    "\n",
    "📚 Step 2: Interpreting Common Language Patterns\n",
    "\n",
    "I grouped trigrams that reflect positive feedback (e.g., “well written”, “easy to follow”, “convincing results”) versus constructive or critical comments (e.g., “limited novelty”, “could authors add”, “would be interesting to see”).\n",
    "\n",
    "From this:\n",
    "\n",
    "    Positive trigrams suggest what reviewers value.\n",
    "\n",
    "    Critical trigrams highlight common reviewer frustrations or unmet expectations.\n",
    "\n",
    "📈 Step 3: Mapping to Author Actions\n",
    "\n",
    "Next, I translated these themes into specific writing or research practices that authors could adopt to improve their papers.\n",
    "\n",
    "For example:\n",
    "\n",
    "    Many trigrams praise clarity: → “Prioritize Clarity and Organization”\n",
    "\n",
    "    Reviewers mention novelty often: → “Emphasize Novelty and Justification”\n",
    "\n",
    "    Common requests like “would be interesting to see” suggest preemptive additions: → “Common Reviewer Requests”\n",
    "\n",
    "Each of these themes was backed by multiple trigram patterns in the file, which increases confidence in the recommendation.\n",
    "\n",
    "🧠 Step 4: Weighing Frequency and Context\n",
    "\n",
    "Not every trigram has equal weight. I considered both:\n",
    "\n",
    "    Raw frequency (e.g., trigrams that appear hundreds of times are more influential).\n",
    "\n",
    "    Contextual relevance (e.g., “method limited novelty” is low frequency but highly specific and critical).\n",
    "\n",
    "Combining these gives a qualitative + quantitative view of how to improve paper writing\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880922ef-1c32-4476-8830-92cfb6ac7deb",
   "metadata": {},
   "source": [
    "# Going to the extreme: a general N-gram approach\n",
    "In the above examples we considered a limited context of 2 and 3 words. However, some may say that this is too little context to actually extract any meaningful info. While the responses we have gotten from ChatGPT may prove this affirmation to be misguided, we may try to increase the context again to see how much this changes the response of the LLM. The extreme case would be to use a single string containing all of the reviews, however we do not think this approach would be beneficial due to the possible hallucinations an LLM could have on a big datatset. A more intelligent approach would be to try and use k-fold cross-validation to test how different types of n-grams behave: while this is a valid approach, we feel like it is a bit of an overkill for the following exercise. Because of this, in this section we limit ourselves to test a 10-grams approach to see if the results differ from the bigrams or the trigrams approach. If this was not to be the case or the results obtained by increasing the context were to be worse than the two previous approaches, this would tell us that increasing the context above a certain threshold does not improve or may even worsen (difficult, but still possible) the obtained results. Note that the following approach has a problem: it is difficult that many 10-grams appear more than once. Hence, while this methodology indeed increases the context, it creates a problem regarding the cleaning of the created 10-grams. If we were to not put a threshold, we would also include useless/uninteresting 10-grams, but, if we put one, then we would probably lose a lot of information. Hence, this approach may not be the most viable option, but, in order to be sure, we still proceed in order to see whether our worries are indeed founded or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b517d8f-d9d6-4c88-8f55-58cadce3052c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tengrams ready!\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "#we create the 10-grams\n",
    "ten_grams = [list(nltk.ngrams(sublist, 10)) for sublist in filtered_reviews]\n",
    "#now we flatten the 10-grams' list to compute the frequency\n",
    "all_ten_grams = [tg for sublist in ten_grams for tg in sublist]\n",
    "#we compute the frequency of each 10-gram\n",
    "fdist = nltk.FreqDist(all_ten_grams)\n",
    "#we set a threshold and throw away all the 10-grams that appear less than it.\n",
    "#note we have chosen a lower threshold due to the difficulty of ten exact words appearing in the same order in multiple points of the dataset\n",
    "threshold = 2 \n",
    "filtered_ten_grams = [(k, v) for k, v in fdist.items() if v >= threshold]\n",
    "#we write the 10-grams onto a file\n",
    "with open('out_ten.txt', 'w') as output:\n",
    "    output.write('\\n'.join('%s %s' % x for x in filtered_ten_grams))\n",
    "    output.close()\n",
    "print(\"Tengrams ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55fc70c-5660-42c0-8bb8-3607aef79a41",
   "metadata": {},
   "source": [
    "We then give the output file generated by the code above to ChatGPT with the following prompt: **\"The following text file contains 10-grams associated with their relative frequency. The following 10-grams come from a series of reviews about scientific papers. Your task is to extrapolate information regarding how could an author write better papers using only the data provided in the text file.\"**. To this request, the LLM answered as such:\n",
    "***\n",
    "Based on the analysis of the 10-grams and their relative frequencies in the file, several recurring themes emerge that suggest how authors can improve their scientific papers. Here are the main takeaways:\n",
    "1. Clarity and Presentation of Ideas\n",
    "\n",
    "Many 10-grams emphasize the importance of clearly presenting ideas:\n",
    "\n",
    "    \"the paper is well written\"\n",
    "\n",
    "    \"is clearly written and easy\"\n",
    "\n",
    "    \"paper is easy to follow\"\n",
    "\n",
    "    \"the writing is clear and\"\n",
    "\n",
    "    \"clarity of the writing and\"\n",
    "\n",
    "🔹 Advice: Focus on clarity in writing. Use precise language, maintain logical flow, and avoid unnecessary jargon. Make your paper accessible to a broader audience without compromising scientific accuracy.\n",
    "2. Organization and Structure\n",
    "\n",
    "Several entries praise good organization and coherence:\n",
    "\n",
    "    \"well organized and clearly written\"\n",
    "\n",
    "    \"the structure of the paper\"\n",
    "\n",
    "    \"is structured in a clear\"\n",
    "\n",
    "🔹 Advice: Structure your paper logically: introduction, methodology, results, discussion, conclusion. Use section headings and transitions to guide the reader.\n",
    "3. Motivation and Contribution\n",
    "\n",
    "Strong papers clearly state their motivation and contributions:\n",
    "\n",
    "    \"clearly stated and the motivation\"\n",
    "\n",
    "    \"the motivation of the paper\"\n",
    "\n",
    "    \"contribution of the paper is\"\n",
    "\n",
    "    \"main contribution of the paper\"\n",
    "\n",
    "🔹 Advice: Early in the paper, explain why the problem is important and how your work contributes new insights or advancements.\n",
    "4. Novelty and Significance\n",
    "\n",
    "Originality is a key factor mentioned:\n",
    "\n",
    "    \"novel and interesting contribution to\"\n",
    "\n",
    "    \"interesting and the contribution is\"\n",
    "\n",
    "    \"the novelty of the paper is\"\n",
    "\n",
    "🔹 Advice: Highlight the novel aspects of your research. Be explicit about what’s new compared to previous work.\n",
    "5. Evaluation and Results\n",
    "\n",
    "Papers with strong evaluations are appreciated:\n",
    "\n",
    "    \"evaluation is thorough and convincing\"\n",
    "\n",
    "    \"results are clearly presented and\"\n",
    "\n",
    "    \"experiments are well designed and\"\n",
    "\n",
    "🔹 Advice: Include detailed, well-structured experiments or case studies. Present results clearly with figures and tables, and compare them against strong baselines.\n",
    "6. Related Work\n",
    "\n",
    "Connections to prior work are important:\n",
    "\n",
    "    \"situates the paper well within\"\n",
    "\n",
    "    \"well motivated and the related\"\n",
    "\n",
    "    \"related work is comprehensive and\"\n",
    "\n",
    "🔹 Advice: Clearly situate your paper within the existing literature. A good related work section shows you understand the field and how your work fits in.\n",
    "7. Reproducibility and Technical Detail\n",
    "\n",
    "Reproducibility adds credibility:\n",
    "\n",
    "    \"technical details are clearly explained\"\n",
    "\n",
    "    \"code and data are available\"\n",
    "\n",
    "🔹 Advice: Provide sufficient detail to replicate results. Share datasets, code, or supplementary material whenever possible.\n",
    "***\n",
    "\n",
    "The response we got, while sharing a lot of entries with the previous approaches, gives us a more general response and one that seems to be adequately justified, due to the improved context. However, note how the response never directly cites an entire 10-grams, but mostly 5-grams, which may show how, while being a viable choice, increasing the context above a certain size does not benefit the final result. According to the answer we got, it seems like 5-grams would be the best approach towards obtaining the desired information, when feeding the data to an LLM. However, there is something extremely wrong with the LLM's response: the cited 5-grams contain also words contained in the stop words list (the most basic example of this is \"a\")! Thus, it's easy to guess that ChatGPT did not only use the provided 10-grams to create its answer, which is not what we want. Because of this, we tried to give it the same input file, but with a modified prompt (that also tries to not make it use any Python code in the process): **\"The following text file contains 10-grams associated with their relative frequency. The following 10-grams come from a series of reviews about scientific papers. Your task is to extrapolate information regarding how could an author write better papers using only the data provided in the text file. I repeat: YOU NEED TO USE ONLY THE 10-GRAMS THAT ARE PRESENT IN THE FILE!!! Also, try not to use any Python code in the process\"**. The following is the obtained answer:<br>\n",
    "\n",
    "***\n",
    "Based solely on the 10-grams provided in the file, several concrete recommendations emerge for how an author could write better scientific papers. These recommendations are not based on inference beyond the n-grams—they are constructed directly from patterns and repeated phrases in the data: <br>\n",
    "1. Clearly State the Decision and Reasoning\n",
    "\n",
    "    N-gram: \"clearly state decision accept reject one two key reasons choice\"\n",
    "\n",
    "    Recommendation: Authors should explicitly state their conclusions and the key reasons behind them. Reviewers value clarity in decision-making and structured argumentation.\n",
    "\n",
    "2. Provide Additional Feedback and Clarify Key Points\n",
    "\n",
    "    N-gram: \"provide additional feedback aim improve paper make clear points help\"\n",
    "\n",
    "    Recommendation: It is crucial to make important points in the paper clear and actionable, especially in response to feedback. Clarifying the paper's contributions improves reviewers’ assessments.\n",
    "\n",
    "3. Support Claims with Rigorous Evidence\n",
    "\n",
    "    N-gram: \"paper support claims includes determining results whether theoretical empirical correct\"\n",
    "\n",
    "    Recommendation: A strong paper backs all claims with either theoretical or empirical validation, ensuring scientific rigor.\n",
    "\n",
    "4. Address Weaknesses to Increase Review Scores\n",
    "\n",
    "    N-gram: \"recommend rejecting paper would willing increase score points addressed sufficient detail\"\n",
    "\n",
    "    Recommendation: Authors should thoroughly address any weaknesses pointed out by reviewers. If major comments are addressed in detail, reviewers might reconsider their scores.\n",
    "\n",
    "5. Evaluate the Effectiveness of Proposed Methods\n",
    "\n",
    "    N-gram: \"evaluate effectiveness defense thoroughly convince readers really works especially robustness\"\n",
    "\n",
    "    Recommendation: Any proposed method should be evaluated comprehensively, especially in terms of robustness, and must convincingly demonstrate that it works under expected challenges.\n",
    "\n",
    "6. Compare Against State-of-the-Art\n",
    "\n",
    "    N-gram: \"compare proposed method state-of-the-art domain adaptation methods based summary\"\n",
    "\n",
    "    Recommendation: Authors should compare their work against the state-of-the-art, ideally in the same domain, to show relevance and progress.\n",
    "\n",
    "7. Ensure Clarity and Organization\n",
    "\n",
    "    N-gram: \"clarity overall paper well organized logically clear images well-presented well-explained\"\n",
    "\n",
    "    Recommendation: The manuscript should be well-structured, with clearly presented figures and explanations. Organization significantly affects readability and impact.\n",
    "\n",
    "8. Anticipate Reviewer Rebuttals\n",
    "\n",
    "    N-gram: \"post rebuttal start reading reviews authors response decided change score\"\n",
    "\n",
    "    Recommendation: A thoughtful and well-crafted author response can sway reviewer scores, so it's essential to respond meaningfully during the rebuttal phase.\n",
    "***\n",
    "\n",
    "The above results indeed make use of the input file provided and, while the LLM's answer seems a bit of a stretch in some instances or are not particularly relevant in our case (like point 4), they also seem to be, overall, acceptable. Moreover, when asked if it used any Python code to complete the task, the LLM denied the claim:\n",
    "\n",
    "***\n",
    "No — I did not use Python code, either explicitly or behind the scenes.\n",
    "\n",
    "Everything was done by:\n",
    "\n",
    "    Manually reviewing the content of the n-grams in the uploaded text file,\n",
    "\n",
    "    Identifying patterns and repetitions, and\n",
    "\n",
    "    Rephrasing those literal phrases into actionable advice, while staying within the boundaries of what the n-grams directly conveyed.\n",
    "\n",
    "In fact, I treated the file content as static text. I didn’t filter, sort, tokenize, or compute anything programmatically. All conclusions are built directly from what's written — no background models, no heuristics, and no data processing.\n",
    "***\n",
    "\n",
    "And, when asked if it really did not use any Pyhton code throughout the task, here is what ChatGPT answered with:\n",
    "\n",
    "***\n",
    "No — and this is important — I did not use Python at all, not even implicitly or behind the scenes.\n",
    "\n",
    "Let me clarify how this works in your case: <br>\n",
    "✅ What Happened:\n",
    "\n",
    "    You uploaded a plain text file.\n",
    "\n",
    "    I (ChatGPT) read the content as-is, as a static document.\n",
    "\n",
    "    I scanned it visually and manually, using natural language understanding (not programming logic).\n",
    "\n",
    "    I did not call any Python tools, nor did I parse, tokenize, count, or sort data programmatically.\n",
    "\n",
    "    There were no hidden scripts, background dataframes, or statistical operations.\n",
    "\n",
    "❌ What I Did Not Do:\n",
    "\n",
    "    I did not use collections.Counter, pandas, regex, or any script to process frequencies.\n",
    "\n",
    "    I did not compute or search using code.\n",
    "\n",
    "    I did not extract or aggregate data automatically — everything was treated as flat, raw text.\n",
    "\n",
    "What I Did Use:\n",
    "\n",
    "Only language comprehension — like you would if you were reading a printed list of phrases and making notes with a pen.\n",
    "***\n",
    "\n",
    "Whether we can really trust the LLM's response is debateable, but it is also extremely difficult to prove otherwise.\n",
    "\n",
    "In this part, we have shown how we can use basic Natural Language Processing techniques to clean the dataset and thus provide a compressed representation of it, which still tries to capture the information present in the original one, to then give as input to an LLM to try and extract information from it. We have proved that, by using these techniques, we are able to correctly extrapolate information from the reduced dataset, thus minimizing the amount of tokens we need to use. We have also been able to show how an increase in context of the n-grams above a certain threshold may not improve the overall results. However, while the results we have achieved seem to be correct or, at least, acceptable, doubts still remain about the possible reasoning of the LLM: in some instances, it seemed like it was spouting pre-conceived facts, instead of trying to extract information from the given input. However, we have no real way to try and test this, so this doubt, while present, is difficult to verify.\n",
    "\n",
    "# Second version: Topic Clustering using SBERT-BERTopic\n",
    "In this part of the exercise, we try to use the cleaned dataset to extract information from it using topic clustering. We attempt to achieve this by using SBERT (https://github.com/UKPLab/sentence-transformers) to translate the reviews into word embeddings and BERTopic (https://github.com/MaartenGr/BERTopic) to perform the topic clustering. We decided to proceed with this method due to the possibility of topic clustering being able to identify common critiques or praises in a more clever way than traditional clustering models. However, this is just a possibility, as it is also equally possible that the model gets tricked by the technical topics discussed in the reviews and completely ignores the topics that would be useful for our objective. Still, the only way we can know whether this is actually going to happen or not is to try and test this approach: if it works, then we will be able to extract various information relevant to our goal; if it doesn't, then it means we either have to clean the dataset to avoid any technical terms - which, however, is not something we can realistically do, as it could lead to worse results than the above approaches, due to some critiques being related to the technical aspects of the works examined - or it means that the topic-based approach will not work in this case and, thus, we would have to go with traditional clustering. Our objective in the following is to reduce the dataset to a set of topics and key words that, when fed to the LLM, can be used to extract information about our objective. Another way to use the LLM in this process is by giving it the list of topics to create custom labels to be used to visualize the data in a more comprehensible way. In this section, we will use a lot of code taken from the following Colab Notebook: https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2?usp=sharing#scrollTo=meEM585mCk7a. <br>\n",
    "**NB:** We recommend using a GPU for this section to speed up the process, due to the use of ML and DL models that is done in the following code. In case no GPU is available, this notebook can still be ran (it is just going to be rather slow) or services such as Google Colab can be used to obtain the necessary resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41ce6620-3261-470a-87e1-95c1ce0bd6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in /home/nyasatte/.local/lib/python3.10/site-packages (4.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (2.7.0)\n",
      "Requirement already satisfied: scikit-learn in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (1.4.1.post1)\n",
      "Requirement already satisfied: scipy in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence-transformers) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers) (59.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nyasatte/.local/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2020.6.20)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bertopic in /home/nyasatte/.local/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (0.8.40)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (2.2.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (6.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (1.4.1.post1)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (4.1.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (4.67.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic) (0.5.7)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.1.5->bertopic) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic) (2024.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (1.37.1)\n",
      "Requirement already satisfied: packaging in /home/nyasatte/.local/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from scikit-learn>=1.0->bertopic) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence-transformers>=0.4.1->bertopic) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.3.0->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (59.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic) (0.5.13)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/nyasatte/.local/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic) (0.44.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nyasatte/.local/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: umap-learn in /home/nyasatte/.local/lib/python3.10/site-packages (0.5.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.3.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn) (1.12.0)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn) (1.4.1.post1)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn) (0.5.13)\n",
      "Requirement already satisfied: tqdm in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn) (4.67.1)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/nyasatte/.local/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn) (0.44.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/nyasatte/.local/lib/python3.10/site-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from scikit-learn>=0.22->umap-learn) (3.6.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bertopic[spacy] in /home/nyasatte/.local/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: hdbscan>=0.8.29 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (0.8.40)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (2.2.3)\n",
      "Requirement already satisfied: plotly>=4.7.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (6.0.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (1.4.1.post1)\n",
      "Requirement already satisfied: sentence-transformers>=0.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (4.1.0)\n",
      "Requirement already satisfied: tqdm>=4.41.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (4.67.1)\n",
      "Requirement already satisfied: umap-learn>=0.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (0.5.7)\n",
      "Requirement already satisfied: spacy>=3.0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from bertopic[spacy]) (3.8.5)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic[spacy]) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from hdbscan>=0.8.29->bertopic[spacy]) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic[spacy]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas>=1.1.5->bertopic[spacy]) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas>=1.1.5->bertopic[spacy]) (2024.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic[spacy]) (1.37.1)\n",
      "Requirement already satisfied: packaging in /home/nyasatte/.local/lib/python3.10/site-packages (from plotly>=4.7.0->bertopic[spacy]) (25.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->bertopic[spacy]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from scikit-learn>=1.0->bertopic[spacy]) (3.6.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (4.51.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (2.7.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (9.0.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sentence-transformers>=0.4.1->bertopic[spacy]) (4.13.2)\n",
      "Requirement already satisfied: filelock in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.18.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2025.3.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (0.15.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy>=3.0.1->bertopic[spacy]) (59.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy>=3.0.1->bertopic[spacy]) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[spacy]) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[spacy]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[spacy]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.0.1->bertopic[spacy]) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nyasatte/.local/lib/python3.10/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2020.6.20)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[spacy]) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy>=3.0.1->bertopic[spacy]) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[spacy]) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[spacy]) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/nyasatte/.local/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy>=3.0.1->bertopic[spacy]) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy>=3.0.1->bertopic[spacy]) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy>=3.0.1->bertopic[spacy]) (0.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=0.4.1->bertopic[spacy]) (1.3.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic[spacy]) (0.61.2)\n",
      "Requirement already satisfied: pynndescent>=0.5 in /home/nyasatte/.local/lib/python3.10/site-packages (from umap-learn>=0.5.0->bertopic[spacy]) (0.5.13)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /home/nyasatte/.local/lib/python3.10/site-packages (from numba>=0.51.2->umap-learn>=0.5.0->bertopic[spacy]) (0.44.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jinja2->spacy>=3.0.1->bertopic[spacy]) (2.1.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/nyasatte/.local/lib/python3.10/site-packages (3.8.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (0.15.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (2.11.4)\n",
      "Requirement already satisfied: jinja2 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nyasatte/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.0.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: wrapt in /home/nyasatte/.local/lib/python3.10/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.nvidia.com\n",
      "Collecting cudf-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting dask-cudf-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/dask-cudf-cu12/dask_cudf_cu12-25.4.0-py3-none-any.whl (50 kB)\n",
      "Collecting cuml-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cuml-cu12/cuml_cu12-25.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cugraph-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cugraph-cu12/cugraph_cu12-25.4.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nx-cugraph-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.4.0-py3-none-any.whl (166 kB)\n",
      "Collecting cuspatial-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cuspatial-cu12/cuspatial_cu12-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (4.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cuproj-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cuproj-cu12/cuproj_cu12-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cuxfilter-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cuxfilter-cu12/cuxfilter_cu12-25.4.0-py3-none-any.whl (83 kB)\n",
      "Collecting cucim-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cucim-cu12/cucim_cu12-25.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pylibraft-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/pylibraft-cu12/pylibraft_cu12-25.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (853 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m853.5/853.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting raft-dask-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/raft-dask-cu12/raft_dask_cu12-25.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (288.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting cuvs-cu12==25.4.*\n",
      "  Downloading https://pypi.nvidia.com/cuvs-cu12/cuvs_cu12-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (730 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m730.1/730.1 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools (from cudf-cu12==25.4.*)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting cuda-python<13.0a0,>=12.6.2 (from cudf-cu12==25.4.*)\n",
      "  Downloading cuda_python-12.8.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting cupy-cuda12x>=12.0.0 (from cudf-cu12==25.4.*)\n",
      "  Downloading cupy_cuda12x-13.4.1-cp310-cp310-manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (2025.3.2)\n",
      "Collecting libcudf-cu12==25.4.* (from cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.4.0-py3-none-manylinux_2_28_x86_64.whl (565.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.0/565.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:03\u001b[0m\n",
      "\u001b[?25hCollecting numba-cuda<0.5.0a0,>=0.4.0 (from cudf-cu12==25.4.*)\n",
      "  Downloading numba_cuda-0.4.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: numba<0.62.0a0,>=0.59.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (0.61.2)\n",
      "Requirement already satisfied: numpy<3.0a0,>=1.23 in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (1.26.4)\n",
      "Collecting nvtx>=0.2.1 (from cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/nvtx/nvtx-0.2.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (474 kB)\n",
      "Requirement already satisfied: packaging in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (25.0)\n",
      "Requirement already satisfied: pandas<2.2.4dev0,>=2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (2.2.3)\n",
      "Collecting pyarrow<20.0.0a0,>=14.0.0 (from cudf-cu12==25.4.*)\n",
      "  Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pylibcudf-cu12==25.4.* (from cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.7 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pynvjitlink-cu12 (from cudf-cu12==25.4.*)\n",
      "  Downloading pynvjitlink_cu12-0.6.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: rich in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (14.0.0)\n",
      "Collecting rmm-cu12==25.4.* (from cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/rmm-cu12/rmm_cu12-25.4.0-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from cudf-cu12==25.4.*) (4.13.2)\n",
      "Collecting pynvml<13.0.0a0,>=12.0.0 (from dask-cudf-cu12==25.4.*)\n",
      "  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting rapids-dask-dependency==25.4.* (from dask-cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/rapids-dask-dependency/rapids_dask_dependency-25.4.0-py3-none-any.whl (18 kB)\n",
      "Collecting dask-cuda==25.4.* (from cuml-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/dask-cuda/dask_cuda-25.4.0-py3-none-any.whl (135 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (1.3.2)\n",
      "Collecting libcuml-cu12==25.4.* (from cuml-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libcuml-cu12/libcuml_cu12-25.4.0-py3-none-manylinux_2_28_x86_64.whl (424.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m424.6/424.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hCollecting numba<0.62.0a0,>=0.59.1 (from cudf-cu12==25.4.*)\n",
      "  Downloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (12.5.4.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from cuml-cu12==25.4.*) (1.12.0)\n",
      "Collecting treelite==4.4.1 (from cuml-cu12==25.4.*)\n",
      "  Downloading treelite-4.4.1-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting libcugraph-cu12==25.4.* (from cugraph-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libcugraph-cu12/libcugraph_cu12-25.4.1-py3-none-manylinux_2_28_x86_64.whl (1496.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 GB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0m\n",
      "\u001b[?25hCollecting pylibcugraph-cu12==25.4.* (from cugraph-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/pylibcugraph-cu12/pylibcugraph_cu12-25.4.1-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting ucx-py-cu12==0.43.* (from cugraph-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/ucx-py-cu12/ucx_py_cu12-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx>=3.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from nx-cugraph-cu12==25.4.*) (3.4.2)\n",
      "Collecting geopandas>=1.0.0 (from cuspatial-cu12==25.4.*)\n",
      "  Downloading geopandas-1.0.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting libcuspatial-cu12==25.4.* (from cuspatial-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libcuspatial-cu12/libcuspatial_cu12-25.4.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (32.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.6/32.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting bokeh<=3.6.3,>=3.1 (from cuxfilter-cu12==25.4.*)\n",
      "  Downloading bokeh-3.6.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting datashader>=0.15 (from cuxfilter-cu12==25.4.*)\n",
      "  Downloading datashader-0.18.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting holoviews>=1.16.0 (from cuxfilter-cu12==25.4.*)\n",
      "  Downloading holoviews-1.20.2-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting jupyter-server-proxy (from cuxfilter-cu12==25.4.*)\n",
      "  Downloading jupyter_server_proxy-4.4.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting panel>=1.0 (from cuxfilter-cu12==25.4.*)\n",
      "  Downloading panel-1.6.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting shapely<2.1.0 (from cuxfilter-cu12==25.4.*)\n",
      "  Downloading shapely-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: click in /home/nyasatte/.local/lib/python3.10/site-packages (from cucim-cu12==25.4.*) (8.1.7)\n",
      "Collecting lazy-loader>=0.4 (from cucim-cu12==25.4.*)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting scikit-image<0.26.0a0,>=0.19.0 (from cucim-cu12==25.4.*)\n",
      "  Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting libraft-cu12==25.4.* (from pylibraft-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libraft-cu12/libraft_cu12-25.4.0-py3-none-manylinux_2_28_x86_64.whl (21.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/21.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting distributed-ucxx-cu12==0.43.* (from raft-dask-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/distributed-ucxx-cu12/distributed_ucxx_cu12-0.43.0-py3-none-any.whl (25 kB)\n",
      "Collecting libcuvs-cu12==25.4.* (from cuvs-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libcuvs-cu12/libcuvs_cu12-25.4.0-py3-none-manylinux_2_28_x86_64.whl (1182.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 GB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:05\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Jinja2>=2.9 in /home/nyasatte/.local/lib/python3.10/site-packages (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*) (3.1.6)\n",
      "Requirement already satisfied: contourpy>=1.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*) (1.2.0)\n",
      "Requirement already satisfied: pillow>=7.1.0 in /usr/lib/python3/dist-packages (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*) (9.0.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/lib/python3/dist-packages (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*) (5.4.1)\n",
      "Requirement already satisfied: tornado>=6.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*) (6.4.2)\n",
      "Collecting xyzservices>=2021.09.1 (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*)\n",
      "  Downloading xyzservices-2025.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting cuda-bindings~=12.8.0 (from cuda-python<13.0a0,>=12.6.2->cudf-cu12==25.4.*)\n",
      "  Downloading cuda_bindings-12.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting zict>=2.0.0 (from dask-cuda==25.4.*->cuml-cu12==25.4.*)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting ucxx-cu12==0.43.* (from distributed-ucxx-cu12==0.43.*->raft-dask-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/ucxx-cu12/ucxx_cu12-0.43.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (727 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.3/727.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting libkvikio-cu12==25.4.* (from libcudf-cu12==25.4.*->cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libkvikio-cu12/libkvikio_cu12-25.4.0-py3-none-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting librmm-cu12==25.4.* (from libcudf-cu12==25.4.*->cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/librmm-cu12/librmm_cu12-25.4.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvcomp-cu12==4.2.0.11 (from libcudf-cu12==25.4.*->cudf-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/nvidia-nvcomp-cu12/nvidia_nvcomp_cu12-4.2.0.11-py3-none-manylinux_2_28_x86_64.whl (46.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting rapids-logger==0.1.* (from libcudf-cu12==25.4.*->cudf-cu12==25.4.*)\n",
      "  Downloading rapids_logger-0.1.1-py3-none-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba<0.62.0a0,>=0.59.1->cudf-cu12==25.4.*)\n",
      "  Downloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.4.*) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.4.*) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nyasatte/.local/lib/python3.10/site-packages (from pandas<2.2.4dev0,>=2.0->cudf-cu12==25.4.*) (2024.1)\n",
      "Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml<13.0.0a0,>=12.0.0->dask-cudf-cu12==25.4.*)\n",
      "  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting dask==2025.2.0 (from rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading dask-2025.2.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting distributed==2025.2.0 (from rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading distributed-2025.2.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting cloudpickle>=3.0.0 (from dask==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting partd>=1.4.0 (from dask==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting toolz>=0.10.0 (from dask==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting importlib_metadata>=4.13.0 (from dask==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting locket>=1.0.0 (from distributed==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting msgpack>=1.0.2 (from distributed==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from distributed==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*) (7.0.0)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading tblib-3.1.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in /usr/lib/python3/dist-packages (from distributed==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*) (1.26.5)\n",
      "Collecting pillow>=7.1.0 (from bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*)\n",
      "  Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /home/nyasatte/.local/lib/python3.10/site-packages (from scikit-image<0.26.0a0,>=0.19.0->cucim-cu12==25.4.*) (2.34.0)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image<0.26.0a0,>=0.19.0->cucim-cu12==25.4.*)\n",
      "  Using cached tifffile-2025.3.30-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting libucx-cu12<1.19,>=1.15.0 (from ucx-py-cu12==0.43.*->cugraph-cu12==25.4.*)\n",
      "  Downloading libucx_cu12-1.18.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.9 kB)\n",
      "Collecting libucxx-cu12==0.43.* (from ucxx-cu12==0.43.*->distributed-ucxx-cu12==0.43.*->raft-dask-cu12==25.4.*)\n",
      "  Downloading https://pypi.nvidia.com/libucxx-cu12/libucxx_cu12-0.43.0-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (515 kB)\n",
      "Collecting fastrlock>=0.5 (from cupy-cuda12x>=12.0.0->cudf-cu12==25.4.*)\n",
      "  Downloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting colorcet (from datashader>=0.15->cuxfilter-cu12==25.4.*)\n",
      "  Downloading colorcet-3.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting multipledispatch (from datashader>=0.15->cuxfilter-cu12==25.4.*)\n",
      "  Downloading multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting param (from datashader>=0.15->cuxfilter-cu12==25.4.*)\n",
      "  Downloading param-2.2.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting pyct (from datashader>=0.15->cuxfilter-cu12==25.4.*)\n",
      "  Downloading pyct-0.5.0-py2.py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader>=0.15->cuxfilter-cu12==25.4.*) (2.32.3)\n",
      "Collecting xarray (from datashader>=0.15->cuxfilter-cu12==25.4.*)\n",
      "  Downloading xarray-2025.4.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*)\n",
      "  Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=0.6.0->cugraph-cu12==25.4.*) (3.3)\n",
      "Collecting pyogrio>=0.7.2 (from geopandas>=1.0.0->cuspatial-cu12==25.4.*)\n",
      "  Downloading pyogrio-0.10.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting pyproj>=3.3.0 (from geopandas>=1.0.0->cuspatial-cu12==25.4.*)\n",
      "  Downloading pyproj-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting pyviz-comms>=2.1 (from holoviews>=1.16.0->cuxfilter-cu12==25.4.*)\n",
      "  Downloading pyviz_comms-3.0.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata>=4.13.0->dask==2025.2.0->rapids-dask-dependency==25.4.*->dask-cudf-cu12==25.4.*)\n",
      "  Downloading zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from Jinja2>=2.9->bokeh<=3.6.3,>=3.1->cuxfilter-cu12==25.4.*) (2.1.5)\n",
      "Requirement already satisfied: bleach in /home/nyasatte/.local/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==25.4.*) (6.1.0)\n",
      "Collecting linkify-it-py (from panel>=1.0->cuxfilter-cu12==25.4.*)\n",
      "  Downloading linkify_it_py-2.0.3-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting markdown (from panel>=1.0->cuxfilter-cu12==25.4.*)\n",
      "  Downloading markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: markdown-it-py in /home/nyasatte/.local/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==25.4.*) (3.0.0)\n",
      "Collecting mdit-py-plugins (from panel>=1.0->cuxfilter-cu12==25.4.*)\n",
      "  Downloading mdit_py_plugins-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: tqdm in /home/nyasatte/.local/lib/python3.10/site-packages (from panel>=1.0->cuxfilter-cu12==25.4.*) (4.67.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from pyogrio>=0.7.2->geopandas>=1.0.0->cuspatial-cu12==25.4.*) (2020.6.20)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.4dev0,>=2.0->cudf-cu12==25.4.*) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /home/nyasatte/.local/lib/python3.10/site-packages (from bleach->panel>=1.0->cuxfilter-cu12==25.4.*) (0.5.1)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.15.0)\n",
      "Collecting simpervisor>=1.0.0 (from jupyter-server-proxy->cuxfilter-cu12==25.4.*)\n",
      "  Downloading simpervisor-1.0.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server-proxy->cuxfilter-cu12==25.4.*) (5.14.3)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (4.9.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (7.7.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.21.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (26.4.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.8.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/nyasatte/.local/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.3.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/nyasatte/.local/lib/python3.10/site-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (21.2.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (4.3.7)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (4.23.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.0.7)\n",
      "Requirement already satisfied: referencing in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.36.2)\n",
      "Requirement already satisfied: rfc3339-validator in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.1.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2023.12.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.24.0)\n",
      "Requirement already satisfied: fqdn in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (3.0.0)\n",
      "Requirement already satisfied: uri-template in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (24.11.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (4.12.3)\n",
      "Requirement already satisfied: defusedxml in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.19.1)\n",
      "Requirement already satisfied: tinycss2<1.3,>=1.1.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /home/nyasatte/.local/lib/python3.10/site-packages (from nbformat>=5.3.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.21.1)\n",
      "Requirement already satisfied: ptyprocess in /usr/lib/python3/dist-packages (from terminado>=0.8.3->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (0.7.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /home/nyasatte/.local/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.3.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/nyasatte/.local/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in /home/nyasatte/.local/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy->cuxfilter-cu12==25.4.*) (2.9.0.20241206)\n",
      "Collecting uc-micro-py (from linkify-it-py->panel>=1.0->cuxfilter-cu12==25.4.*)\n",
      "  Downloading uc_micro_py-1.0.3-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/nyasatte/.local/lib/python3.10/site-packages (from markdown-it-py->panel>=1.0->cuxfilter-cu12==25.4.*) (0.1.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/nyasatte/.local/lib/python3.10/site-packages (from nvidia-cufft-cu12->cuml-cu12==25.4.*) (12.6.85)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/nyasatte/.local/lib/python3.10/site-packages (from requests->datashader>=0.15->cuxfilter-cu12==25.4.*) (3.3.2)\n",
      "Downloading treelite-4.4.1-py3-none-manylinux2014_x86_64.whl (922 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m922.8/922.8 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bokeh-3.6.3-py3-none-any.whl (6.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading cuda_python-12.8.0-py3-none-any.whl (11 kB)\n",
      "Downloading cuda_bindings-12.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.4/11.4 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.60.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (43.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba_cuda-0.4.0-py3-none-any.whl (453 kB)\n",
      "Downloading pyarrow-19.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n",
      "Downloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n",
      "Downloading dask-2025.2.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading distributed-2025.2.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapids_logger-0.1.1-py3-none-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (192 kB)\n",
      "Downloading scikit_image-0.25.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shapely-2.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading libucx_cu12-1.18.1-py3-none-manylinux_2_28_x86_64.whl (27.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.7/27.7 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:02\u001b[0m\n",
      "\u001b[?25hDownloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading cupy_cuda12x-13.4.1-cp310-cp310-manylinux2014_x86_64.whl (104.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datashader-0.18.0-py3-none-any.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading fastrlock-0.8.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (53 kB)\n",
      "Using cached aiohttp-3.11.18-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (219 kB)\n",
      "Using cached yarl-1.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (333 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
      "Downloading geopandas-1.0.1-py3-none-any.whl (323 kB)\n",
      "Downloading holoviews-1.20.2-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading param-2.2.0-py3-none-any.whl (119 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n",
      "Downloading panel-1.6.3-py3-none-any.whl (28.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.2/28.2 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Using cached pillow-11.2.1-cp310-cp310-manylinux_2_28_x86_64.whl (4.6 MB)\n",
      "Using cached propcache-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (206 kB)\n",
      "Downloading pyogrio-0.10.0-cp310-cp310-manylinux_2_28_x86_64.whl (23.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.9/23.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyproj-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyviz_comms-3.0.4-py3-none-any.whl (83 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading tblib-3.1.0-py3-none-any.whl (12 kB)\n",
      "Using cached tifffile-2025.3.30-py3-none-any.whl (226 kB)\n",
      "Downloading toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading xyzservices-2025.4.0-py3-none-any.whl (90 kB)\n",
      "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "Downloading zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading colorcet-3.1.0-py3-none-any.whl (260 kB)\n",
      "Downloading jupyter_server_proxy-4.4.0-py3-none-any.whl (37 kB)\n",
      "Downloading simpervisor-1.0.0-py3-none-any.whl (8.3 kB)\n",
      "Downloading linkify_it_py-2.0.3-py3-none-any.whl (19 kB)\n",
      "Downloading markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading mdit_py_plugins-0.4.2-py3-none-any.whl (55 kB)\n",
      "Downloading multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading pynvjitlink_cu12-0.6.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (46.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading uc_micro_py-1.0.3-py3-none-any.whl (6.2 kB)\n",
      "Downloading xarray-2025.4.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sortedcontainers, nvtx, nvidia-ml-py, multipledispatch, libkvikio-cu12, fastrlock, cuda-bindings, zipp, zict, xyzservices, uc-micro-py, toolz, tifffile, tblib, simpervisor, shapely, rapids-logger, pyproj, pyogrio, pynvml, pynvjitlink-cu12, pyarrow, propcache, pillow, param, nvidia-nvcomp-cu12, multidict, msgpack, markdown, locket, llvmlite, libucx-cu12, lazy-loader, frozenlist, cupy-cuda12x, cuda-python, colorcet, cloudpickle, cachetools, async-timeout, aiohappyeyeballs, yarl, ucx-py-cu12, treelite, pyviz-comms, pyct, partd, numba, mdit-py-plugins, linkify-it-py, librmm-cu12, importlib_metadata, cuproj-cu12, aiosignal, xarray, scikit-image, rmm-cu12, numba-cuda, libucxx-cu12, libraft-cu12, libcudf-cu12, geopandas, dask, bokeh, aiohttp, ucxx-cu12, pylibraft-cu12, pylibcudf-cu12, panel, libcuvs-cu12, libcuspatial-cu12, libcugraph-cu12, distributed, datashader, cucim-cu12, rapids-dask-dependency, pylibcugraph-cu12, libcuml-cu12, holoviews, cuvs-cu12, cudf-cu12, nx-cugraph-cu12, distributed-ucxx-cu12, dask-cudf-cu12, dask-cuda, cuspatial-cu12, raft-dask-cu12, jupyter-server-proxy, cuml-cu12, cugraph-cu12, cuxfilter-cu12\n",
      "\u001b[2K  Attempting uninstall: llvmlite━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/91\u001b[0m [markdown]gpack]vcomp-cu12]\n",
      "\u001b[2K    Found existing installation: llvmlite 0.44.0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/91\u001b[0m [markdown]\n",
      "\u001b[2K    Uninstalling llvmlite-0.44.0:\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/91\u001b[0m [markdown]\n",
      "\u001b[2K      Successfully uninstalled llvmlite-0.44.0[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/91\u001b[0m [markdown]\n",
      "\u001b[2K  Attempting uninstall: numba━━━━━━━━━━━\u001b[0m\u001b[38;5;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/91\u001b[0m [pyviz-comms]cet]12x]\n",
      "\u001b[2K    Found existing installation: numba 0.61.25;237m╺\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44/91\u001b[0m [pyviz-comms]\n",
      "\u001b[2K    Uninstalling numba-0.61.2:━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47/91\u001b[0m [numba]\n",
      "\u001b[2K      Successfully uninstalled numba-0.61.238;2;249;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47/91\u001b[0m [numba]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91/91\u001b[0m [cuxfilter-cu12]1\u001b[0m [cuxfilter-cu12]cuml-cu12]cu12]]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.18 aiosignal-1.3.2 async-timeout-5.0.1 bokeh-3.6.3 cachetools-5.5.2 cloudpickle-3.1.1 colorcet-3.1.0 cucim-cu12-25.4.0 cuda-bindings-12.8.0 cuda-python-12.8.0 cudf-cu12-25.4.0 cugraph-cu12-25.4.1 cuml-cu12-25.4.0 cuproj-cu12-25.4.0 cupy-cuda12x-13.4.1 cuspatial-cu12-25.4.0 cuvs-cu12-25.4.0 cuxfilter-cu12-25.4.0 dask-2025.2.0 dask-cuda-25.4.0 dask-cudf-cu12-25.4.0 datashader-0.18.0 distributed-2025.2.0 distributed-ucxx-cu12-0.43.0 fastrlock-0.8.3 frozenlist-1.6.0 geopandas-1.0.1 holoviews-1.20.2 importlib_metadata-8.7.0 jupyter-server-proxy-4.4.0 lazy-loader-0.4 libcudf-cu12-25.4.0 libcugraph-cu12-25.4.1 libcuml-cu12-25.4.0 libcuspatial-cu12-25.4.0 libcuvs-cu12-25.4.0 libkvikio-cu12-25.4.0 libraft-cu12-25.4.0 librmm-cu12-25.4.0 libucx-cu12-1.18.1 libucxx-cu12-0.43.0 linkify-it-py-2.0.3 llvmlite-0.43.0 locket-1.0.0 markdown-3.8 mdit-py-plugins-0.4.2 msgpack-1.1.0 multidict-6.4.3 multipledispatch-1.0.0 numba-0.60.0 numba-cuda-0.4.0 nvidia-ml-py-12.570.86 nvidia-nvcomp-cu12-4.2.0.11 nvtx-0.2.11 nx-cugraph-cu12-25.4.0 panel-1.6.3 param-2.2.0 partd-1.4.2 pillow-11.2.1 propcache-0.3.1 pyarrow-19.0.1 pyct-0.5.0 pylibcudf-cu12-25.4.0 pylibcugraph-cu12-25.4.1 pylibraft-cu12-25.4.0 pynvjitlink-cu12-0.6.0 pynvml-12.0.0 pyogrio-0.10.0 pyproj-3.7.1 pyviz-comms-3.0.4 raft-dask-cu12-25.4.0 rapids-dask-dependency-25.4.0 rapids-logger-0.1.1 rmm-cu12-25.4.0 scikit-image-0.25.2 shapely-2.0.7 simpervisor-1.0.0 sortedcontainers-2.4.0 tblib-3.1.0 tifffile-2025.3.30 toolz-1.0.0 treelite-4.4.1 uc-micro-py-1.0.3 ucx-py-cu12-0.43.0 ucxx-cu12-0.43.0 xarray-2025.4.0 xyzservices-2025.4.0 yarl-1.20.0 zict-3.0.0 zipp-3.21.0\n"
     ]
    }
   ],
   "source": [
    "#install the dependencies, if requirements are not already satisfied\n",
    "!pip install -U sentence-transformers\n",
    "!pip install bertopic\n",
    "!pip install umap-learn\n",
    "!pip install bertopic[spacy]\n",
    "!pip install spacy\n",
    "!pip install \\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    \"cudf-cu12==25.4.*\" \"dask-cudf-cu12==25.4.*\" \"cuml-cu12==25.4.*\" \\\n",
    "    \"cugraph-cu12==25.4.*\" \"nx-cugraph-cu12==25.4.*\" \"cuspatial-cu12==25.4.*\" \\\n",
    "    \"cuproj-cu12==25.4.*\" \"cuxfilter-cu12==25.4.*\" \"cucim-cu12==25.4.*\" \\\n",
    "    \"pylibraft-cu12==25.4.*\" \"raft-dask-cu12==25.4.*\" \"cuvs-cu12==25.4.*\" \\\n",
    "    \"nx-cugraph-cu12==25.4.*\"\n",
    "#!pip install openai #uncomment if you want to use ChatGPT from the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59fa57ec-42fc-4e0d-ae62-d1fea484f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " this paper proposes recency bias, an adaptive mini batch selection method for training deep neural networks.\n",
      "['paper', 'proposes', 'recency', 'bias,', 'adaptive', 'mini', 'batch', 'selection', 'method', 'training', 'deep', 'neural', 'networks.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████████████████████████| 5275/5275 [36:47<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168790, 384)\n"
     ]
    }
   ],
   "source": [
    "#import openai #uncomment if you want to use ChatGPT from code\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#divide the reviews in sentences\n",
    "sentences = [sent_tokenize(info.lower()) for info in reviews] \n",
    "#take individual sentences from the reviews divided in sentences\n",
    "intermediate_review_sentences = [sentence for doc in sentences for sentence in doc] \n",
    "#We filter the reviews, excluding the stop words we defined in the first strategy\n",
    "filtered_reviews_sentences = [[word for word in sentence.split() if word not in stop_words] for sentence in intermediate_review_sentences]\n",
    "#we flatten the list to give it as input to the transformer\n",
    "final_sentences = [\" \".join(words) for words in filtered_reviews_sentences]\n",
    "\n",
    "#create the word embeddings\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(final_sentences, show_progress_bar=True)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba1679-65fb-4a03-a9d3-78bae5f1f76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-04 18:07:24,846 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-05-04 18:14:31,990 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-05-04 18:14:32,000 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from bertopic.representation import OpenAI, MaximalMarginalRelevance\n",
    "#from umap import UMAP #uncomment to not use the GPU - remember to comment the corresponding cuml library!\n",
    "#from hdbscan import HDBSCAN #uncomment to not use the GPU - remember to comment the corresponding cuml library!\n",
    "from cuml.cluster import HDBSCAN #uncomment if you want to use the GPU acceleration\n",
    "from cuml.manifold import UMAP #uncomment if you want to use the GPU acceleration\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "#pio.renderers.default = 'colab' #If using Colab, please decomment this line to make the graphs be visible\n",
    "\n",
    "#comment the following line to enable stochastic behaviour (more info at: https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2?usp=sharing#scrollTo=28_EVoOfyZLb)\n",
    "#tuning the hyperparameters could improve the efficiency, but we won't do it, we'll just showcase the idea\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "\n",
    "#we use MMR to make custom labels. We could use ChatGPT to create better custom labels (as it is shown in https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2?usp=sharing)\n",
    "#but, unfortunately, we don't have an API key to use in order to do it...\n",
    "#If you want to use these last labels, then decomment the line related to the use of ChatGPT\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
    "\n",
    "#prompt taken from https://colab.research.google.com/drive/1BoQ_vakEVtojsd2x_U6-_x52OOuqruj2?usp=sharing#scrollTo=28_EVoOfyZLb\n",
    "#prompt = \"\"\"\n",
    "#I have a topic that contains the following documents:\n",
    "#[DOCUMENTS]\n",
    "#The topic is described by the following keywords: [KEYWORDS]\n",
    "#Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "#topic: <topic label>\n",
    "#\"\"\"\n",
    "#client = openai.OpenAI(api_key=\"sk-...\")\n",
    "#openai_model = OpenAI(client, model=\"gpt-o3\", exponential_backoff=True, chat=True, prompt=prompt) #remember in case to change the model to the one you want to use!\n",
    "\n",
    "#we redefine the stop words to use them with the vectorizer model\n",
    "stop_words_manual = set(stopwords.words('english'))\n",
    "#add to the stop_words the refuses introduced while reading the file. We also include punctuation and words like \"et al\" that are used only for referencing to other papers\n",
    "stop_words_manual.update([\"review\", \"#\", \".\", \":\", \";\", \",\", \"-\", \"?\", \"!\", \"(\", \")\", \"[\", \"]\", \"'\", \"=\", \"*\", \"+\", \"%\", \"/\", \"{\", \"}\", \"--\", \"...\", \"&\", \"et\", \"al\", \"<\", \">\", \">=\", \"<=\"])\n",
    "#we use n-grams to reduce the representation of the data\n",
    "vectorizer_model = CountVectorizer(stop_words=list(stop_words_manual), min_df=2, ngram_range=(1, 5))\n",
    "\n",
    "#we declare the clustering algorithms used. Here we will showcase two methods:\n",
    "#  1: HDBScan\n",
    "#  2: Mini Batch K-means\n",
    "kmeans_model = MiniBatchKMeans(n_clusters=100, random_state=42) #note that changing the number of clusters may improve/worsen the overall results. Also, eliminate random_state to enable stochastic behaviour\n",
    "hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True, prediction_data=True)\n",
    "\n",
    "representation_model = {\n",
    "    # \"OpenAI\": openai_model,  #Uncomment if you want to use ChatGPT to create custom labels\n",
    "    \"MMR\": mmr_model\n",
    "}\n",
    "\n",
    "#import the BERTopic model and train it on the embeddings we previously created. We create two versions: one using HDBScan and one using Mini Batch K-means\n",
    "\n",
    "#HDBSCAN model\n",
    "#topic_model = BERTopic(language=\"english\", embedding_model=model, umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model, representation_model=representation_model, calculate_probabilities=True, verbose=True)\n",
    "\n",
    "#MiniBatchKMeans model\n",
    "topic_model = BERTopic(language=\"english\", embedding_model=model, umap_model=umap_model,hdbscan_model=kmeans_model, vectorizer_model=vectorizer_model, representation_model=representation_model, calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(final_sentences, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b894b7c-9c3a-49ab-a2de-6c7ba304bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we assign MMR custom labels\n",
    "mmr_topic_labels = {topic: \" | \".join(list(zip(*values))[0][:3]) for topic, values in topic_model.topic_aspects_[\"MMR\"].items()}\n",
    "topic_model.set_topic_labels(mmr_topic_labels)\n",
    "\n",
    "#Uncomment if you want to use ChatGPT labels\n",
    "#chatgpt_topic_labels = {topic: \" | \".join(list(zip(*values))[0]) for topic, values in topic_model.topic_aspects_[\"OpenAI\"].items()}\n",
    "#chatgpt_topic_labels[-1] = \"Outlier Topic\"\n",
    "#topic_model.set_topic_labels(chatgpt_topic_labels)\n",
    "\n",
    "#we get every single topic and save the corresponding dataframe in a .csv file\n",
    "topic_info = topic_model.get_topic_info()\n",
    "print(topic_info)\n",
    "topic_info.to_csv(\"out_topic.csv\")\n",
    "#if using Colab, uncomment the following lines to save the output file onto your Google Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "# copy it there\n",
    "#!cp out_topic.csv /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7f8d8-e565-4cdf-aef7-a44df9a4ba23",
   "metadata": {},
   "source": [
    "### Side Note: Visualization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002323be-819e-4b68-98a7-e4a52e5a5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to try and visualize the data, here are some ways to do it\n",
    "#VISUALIZATION TECHNIQUE 1: hierarchy\n",
    "\n",
    "#visualize the hirerarchy\n",
    "#topic_model.visualize_hierarchy(custom_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea88917-39c7-4939-bebd-0b7988540501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#VISUALIZATION TECHNIQUE 2: Interactive visualization (NB: note that this visualization is worse than the above one)\n",
    "\n",
    "#we reduce the dimensionality of the embeddings for representation's sake\n",
    "#reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='euclidean').fit_transform(embeddings)\n",
    "\n",
    "#Visualize the documents in 2-dimensional space and show the titles on hover instead of the abstracts\n",
    "#topic_model.visualize_documents([\"n\" for n in range(0,len(reduced_embeddings)], reduced_embeddings=reduced_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7762c7-d43f-451d-814a-eaeeb3f83edd",
   "metadata": {},
   "source": [
    "While the above method is an interesting outlook on how to use DL clustering models to automatically extract information out of the dataset, the problem here lies with our use of a Topic based model, which may get swindled by the various technical words used in the reviews instead of focusing on topics that would be useful for our objective. We tried to combat this by giving as input to the model the individual sentences and not the reviews - this due to the fact that, if reviews were to be used as the input, the model would just find the topic of the paper for which the review was for, completely ignoring our objective in the process. This was done to try and limit the context so that the model may be able to pick up critiques and praises to the papers (ex: from a phrase like \"the paper is weak because the experiments are not clear\" we may be able to extract the notion of weakness due to unclear experiments). In the following we present the results obtained on 6 different approaches: one involving HDBScan, one involving k-means using only 10 clusters, one using 100 clusters, one using 250 clusters, another using 500 clusters and, finally, one using 1000 clusters. <br>\n",
    "We gave the CSV file we obtained as illustrated before to ChatGPT with the following prompt: **\"The following CSV file contains the results of a clustering process done on a series of reviews about scientific papers. It contains a general index, the index of the topic, its name, its representation, its MMR representation and the representative docs. Your task is to extrapolate information regarding how could an author write better papers using only the data provided in the CSV file. Also, please analyse the ENTIRE file\"**. Note that the above prompt leaves space for the LLM to choose the way it wants to achieve the proposed objective. A more stringent prompt may be proposed to extract info only from specific info (like the MMR labels), but our reason for keeping the prompt vague was to see whether the machine was able to autonomously find satisfactory answers without a stringent human supervision on how to do this. Having said this, the following are the results we obtained for every single approach with our comment about the obtained data.\n",
    "\n",
    "## Results: HDBScan method\n",
    "Adversarial Robustness Is a Hot Topic:\n",
    "\n",
    "    Frequent keywords like adversarial, robustness, and adversarial examples show reviewer interest.\n",
    "\n",
    "    ✅ Tip: If relevant, show how your method handles adversarial conditions or contributes to robustness.\n",
    "\n",
    "Write with Clarity and Precision:\n",
    "\n",
    "    Keywords like well written appear directly in topic labels, reflecting reviewer praise.\n",
    "\n",
    "    ✅ Tip: Aim for logical structure and plain language. Avoid unnecessary jargon.\n",
    "\n",
    "Use Strong Visual and Analytical Tools:\n",
    "\n",
    "    Frequent mentions of image, detection, and graph suggest the value of clear visuals and analysis.\n",
    "\n",
    "    ✅ Tip: Use figures and diagrams effectively. Graph-based analysis is appreciated in many domains.\n",
    "\n",
    "Avoid Redundancy or Irrelevant Citations:\n",
    "\n",
    "    Many MMR entries are repetitive citation fragments (e.g., 2018 2018 2018).\n",
    "\n",
    "    ⚠️ Tip: Ensure citations are cleanly formatted and relevant. Avoid citation stuffing.\n",
    "\n",
    "Ground Work in Recognized Fields:\n",
    "\n",
    "    MMR mentions of NeurIPS, program synthesis, and gradient descent show where reviewer attention lies.\n",
    "\n",
    "    ✅ Tip: Position your work clearly within known research areas and explain its novelty.\n",
    "***\n",
    "\n",
    "As it is possible to see, the results are far worse than the ones obtained in the first part, especially for what concerns the LLM's explanation of the items that led it to answer in the way it did, and show how indeed BERTopic was swayed by the more technical topics, as point 1 makes evident enough. There is to say, however, that some entries, like \"Avoid Redundancy or Irrelevant Citations\", are being generated due to a not precise cleaning of the datatset: in an iterative/agile training process, this could be used to refine the dataset to improve the overall results. However, generally, the above results as-is are worse in quality than the previously exposed method and do not justify its use over a far simpler approach such as using n-grams.\n",
    "\n",
    "## Results: MiniBatch K-means - 10 clusters\n",
    "Based on a full analysis of the CSV file containing topic clusters from peer review comments, here are actionable insights that authors can use to write better scientific papers:\n",
    "\n",
    "1. Emphasize Methodological Clarity\n",
    "\n",
    "(Topic 0: training | model)\n",
    "\n",
    "    Keywords: training, model, neural networks, loss, algorithm\n",
    "\n",
    "    Guidance: Ensure detailed explanation of your model's training process. Highlight how your method performs across datasets like MNIST or CIFAR. Make loss functions and optimization strategies transparent.\n",
    "\n",
    "2. Improve Visual Aids and Presentation\n",
    "\n",
    "(Topic 1: figure | table | algorithm)\n",
    "\n",
    "    Keywords: figure, table, algorithm, comparison\n",
    "\n",
    "    Guidance: Reviewers frequently critique unclear or underutilized figures and tables. Ensure visuals directly support your methodology and results, and are explained clearly in the text.\n",
    "\n",
    "3. Focus on Writing Quality and Structure\n",
    "\n",
    "(Topic 2: authors | written | section)\n",
    "\n",
    "    Keywords: written, section, overall, well written\n",
    "\n",
    "    Guidance: Grammar and structure matter. Many reviews explicitly mention whether the paper is well-written. Proofread thoroughly and ensure logical organization of sections.\n",
    "\n",
    "4. Be Rigorous in Adversarial or Specialized Domains\n",
    "\n",
    "(Topic 3: adversarial | paper | images)\n",
    "\n",
    "    Keywords: adversarial, GAN, images, attacks, models\n",
    "\n",
    "    Guidance: For specialized domains like adversarial learning, reviewers expect a deep understanding and relevant state-of-the-art context. Discuss the robustness and generalization of your models.\n",
    "\n",
    "5. Justify Experimental Design and Highlight Novelty\n",
    "\n",
    "(Topic 4: experiments | experimental | novelty)\n",
    "\n",
    "    Keywords: experiments, novelty, evaluation, empirical\n",
    "\n",
    "    Guidance: Reviewers value strong experimental evidence but often question novelty. Make sure your contribution is clearly differentiated from past work, and supported by rigorous, reproducible experiments.\n",
    "\n",
    "6. Explain Attention and Transformer-Based Models Clearly\n",
    "\n",
    "(Topic 5: attention | model | BERT)\n",
    "\n",
    "    Keywords: attention, BERT, embeddings, translation\n",
    "\n",
    "    Guidance: If your work involves transformers or attention mechanisms, provide intuition and ablation studies to show why your approach is better. Clarify complex architectures with diagrams or pseudocode.\n",
    "\n",
    "7. Choose Datasets Thoughtfully\n",
    "\n",
    "(Topic 6: datasets | baselines)\n",
    "\n",
    "    Keywords: MNIST, CIFAR-10, performance, synthetic\n",
    "\n",
    "    Guidance: Common datasets like MNIST and CIFAR-10 are sometimes dismissed as too simple. Include more challenging, real-world datasets or justify their relevance.\n",
    "\n",
    "8. Clearly State Where and How Your Paper Was Published\n",
    "\n",
    "(Topic 7 & 8: ICML/NeurIPS references, arXiv preprints)\n",
    "\n",
    "    Keywords: well written, arxiv preprint, ICML 2019, NeurIPS\n",
    "\n",
    "    Guidance: Reviewers frequently refer to where similar work was published. Cite related work from high-quality venues, and be clear about the status of your submission (e.g., preprint, conference).\n",
    "\n",
    "9. Use Formal Structure and Mathematical Rigor\n",
    "\n",
    "(Topic 9: eq | information processing systems)\n",
    "\n",
    "    Keywords: equation, processing systems, results\n",
    "\n",
    "    Guidance: Include mathematical formulations where appropriate. Use clear and consistent notation, and tie theoretical contributions to practical implications.\n",
    "***\n",
    "\n",
    "The results obtained using 10 clusters are not that much satisfactory: some of them seem like the LLM did a Pindaric flight to arrive there (for example, in point 9), while some other topics are a bit too technical for our purposes (for example, point 6 and 4) and show how BERTopic was fooled by the many technical terms used during the reviews and to which it choose to give more weight rather than to the ones we were interested in.\n",
    "\n",
    "## Results: MiniBatch K-means - 100 clusters\n",
    "Based on the analysis of all 100 clusters in the dataset, here are data-driven suggestions for improving scientific papers:\n",
    "🔍 1. Emphasize Strong Experimental Evidence\n",
    "\n",
    "    Highly frequent terms: experiments, results, performance, accuracy, dataset\n",
    "\n",
    "    Implication: Reviewers consistently value clear, well-executed experimental sections. This includes:\n",
    "\n",
    "        Solid experimental design\n",
    "\n",
    "        Clear reporting of metrics (accuracy, performance)\n",
    "\n",
    "        Comparisons against baselines\n",
    "\n",
    "        Use of diverse datasets\n",
    "\n",
    "✅ Tip: Present detailed, reproducible experiments and explicitly discuss results and their implications.\n",
    "\n",
    "🧠 2. Explain the Proposed Method Clearly\n",
    "\n",
    "    Terms like: proposed, model, method, algorithm, training\n",
    "\n",
    "    Reviewers focus on how well the methodology is described and justified.\n",
    "\n",
    "✅ Tip: Use clear diagrams, step-by-step descriptions, and justify design choices.\n",
    "\n",
    "📄 3. Clarity and Quality of Writing Matters\n",
    "\n",
    "    Phrases like: \"paper well written\", section, written, would\n",
    "\n",
    "    Papers praised for writing quality often score better, even in technical reviews.\n",
    "\n",
    "✅ Tip: Invest in editing for clarity, structure, and logical flow. Reviewers appreciate well-written content.\n",
    "\n",
    "🧪 4. Show Theoretical and Practical Contributions\n",
    "\n",
    "    Keywords such as theoretical, applications, contributions, practical\n",
    "\n",
    "    Reviewers appreciate when theory is backed by practical application or real-world impact.\n",
    "\n",
    "✅ Tip: Discuss both theoretical implications and practical use cases.\n",
    "\n",
    "💡 5. Be Novel, But Not at the Expense of Rigor\n",
    "\n",
    "    Clusters emphasize contributions and novelty (interesting contributions, novel)\n",
    "\n",
    "    However, without rigor (well-validated results), novelty isn't sufficient.\n",
    "\n",
    "✅ Tip: Balance innovation with thorough validation.\n",
    "\n",
    "🧯 6. Address Known Issues and Risks\n",
    "\n",
    "    Frequent terms: adversarial, robustness, uncertainty, limitations\n",
    "\n",
    "    Reviewers care about a paper's handling of known pitfalls in ML models.\n",
    "\n",
    "✅ Tip: Proactively discuss limitations, robustness tests, and failure cases.\n",
    "***\n",
    "\n",
    "As above, the following results are far worse than the ones obtained in the first part, with ChatGPT seemingly inventing relationships between keywords to try and give a complete answer, while lying in the process. At least, unlike the first method, this one did not make the LLM get swayed by technical topics (though the presence of adversarial in point 6 may prove this is not exactly true).\n",
    "\n",
    "## Results: MiniBatch K-means - 250 clusters\n",
    "Most Common Feedback Themes:\n",
    "\n",
    "The most frequent words in representative reviews include:\n",
    "\n",
    "    \"paper\", \"authors\", \"proposed\", \"method\", \"results\", \"model\", \"algorithm\", \"learning\", \"neural\", \"networks\", \"training\"\n",
    "\n",
    "This suggests reviewers focus heavily on:\n",
    "\n",
    "    The clarity and novelty of proposed methods\n",
    "\n",
    "    The quality of results\n",
    "\n",
    "    The modeling approach (especially in deep learning and graph-based models)\n",
    "\n",
    "🧠 How Authors Can Improve Their Papers:\n",
    "\n",
    "1. Clearly Define the Proposed Method\n",
    "\n",
    "    Many reviews emphasize the \"proposed method\". Make sure it's:\n",
    "\n",
    "        Clearly stated\n",
    "\n",
    "        Motivated with a strong rationale\n",
    "\n",
    "        Compared against alternatives\n",
    "\n",
    "2. Improve Explanation and Clarity\n",
    "\n",
    "    Terms like \"information\", \"well\", and \"written\" suggest writing quality is scrutinized.\n",
    "\n",
    "    Ensure:\n",
    "\n",
    "        Logical structure\n",
    "\n",
    "        Clear language (avoid jargon without definitions)\n",
    "\n",
    "        Explicit contribution statements\n",
    "\n",
    "3. Strengthen Results and Evaluation\n",
    "\n",
    "    Frequent mentions of \"results\", \"performance\", and \"experiments\" imply reviewers want:\n",
    "\n",
    "        Strong empirical evidence\n",
    "\n",
    "        Relevant benchmarks\n",
    "\n",
    "        Ablation studies or error analysis\n",
    "\n",
    "4. Address Weaknesses Proactively\n",
    "\n",
    "    Words like \"weak\" indicate reviewers commonly criticize:\n",
    "\n",
    "        Weak baselines\n",
    "\n",
    "        Lack of novelty\n",
    "\n",
    "        Poor generalization\n",
    "\n",
    "To avoid this:\n",
    "\n",
    "    Highlight novel contributions early\n",
    "\n",
    "    Justify your choices (data, baselines, architecture)\n",
    "\n",
    "    Discuss limitations openly\n",
    "\n",
    "5. Use Proper Referencing and Citations\n",
    "\n",
    "    Names like \"zhang\", \"li\", \"chen\" and sites like \"arxiv\" suggest citation quality matters.\n",
    "\n",
    "    Ensure:\n",
    "\n",
    "        Comprehensive and accurate referencing\n",
    "\n",
    "        Avoid overreliance on non-peer-reviewed sources (like arXiv)\n",
    "***\n",
    "\n",
    "This is an exemplifying case of LLMs straight up coming up with their own answers without actually caring about the given data (how could it derive from \"neural network\" that reviewers want quality and novelty is the perfect example of this). In any case, as the above attempts this one seems also not to work due to the model used for clustering focusing too much on technical topics, even though we tried to atomize every phrase to prevent this from happening.\n",
    "\n",
    "## Results: MiniBatch K-means - 500 clusters\n",
    "🔑 Most Emphasized Topics and Phrases (From MMR & Representative Docs)\n",
    "1. Clear Writing and Structure\n",
    "\n",
    "    Frequent phrases: “paper well written,” “well written easy follow,” “writing”\n",
    "\n",
    "    Tip: Prioritize clarity and coherence in writing. Reviewers consistently highlight well-structured, readable papers positively.\n",
    "\n",
    "2. Strong Experimental Validation\n",
    "\n",
    "    Keywords: “experiments,” “ablation study,” “tables,” “plots,” “results”\n",
    "\n",
    "    Tip: Support your claims with comprehensive and clear experimental results, including visualizations and ablation studies.\n",
    "\n",
    "3. Topical Relevance\n",
    "\n",
    "    Hot topics: “adversarial,” “reinforcement learning,” “graph neural networks,” “GANs,” “SGD,” “gradient descent,” “datasets”\n",
    "\n",
    "    Tip: Engaging with trending research areas can increase reviewer interest. Ensure your work contributes meaningfully to these domains if relevant.\n",
    "\n",
    "4. Conference Awareness\n",
    "\n",
    "    MMR terms like: “NeurIPS,” “CVPR,” “ICLR,” “IEEE Conference on Computer Vision”\n",
    "\n",
    "    Tip: Tailor your writing and formatting to the target venue. Reviewers often reference the norms and expectations of specific conferences.\n",
    "\n",
    "5. Citations and Background\n",
    "\n",
    "    Repeated terms: “references,” “previous work,” “theorems”\n",
    "\n",
    "    Tip: Demonstrate strong grounding in related work and provide complete citations. Clearly situate your work in the context of existing literature.\n",
    "\n",
    "6. Constructive Feedback Phrases\n",
    "\n",
    "    Appearing in representative reviews: “think paper,” “weak accept,” “would improve,” “could,” “seems”\n",
    "\n",
    "    Tip: Address ambiguity and potential concerns proactively. Write with a critical eye toward possible reviewer objections.\n",
    "\n",
    "🧠 Writing Strategy Summary (Based on Top Word Frequencies)\n",
    "\n",
    "    Emphasize clarity and readability—reviewers reward well-articulated papers.\n",
    "\n",
    "    Provide strong experimental evidence with clear figures/tables.\n",
    "\n",
    "    Ensure relevance to current research topics and major conferences.\n",
    "\n",
    "    Build upon and cite existing literature rigorously.\n",
    "\n",
    "    Anticipate reviewer feedback and critiques in advance.\n",
    "***\n",
    "\n",
    "This seems to be the best answer between the ones using k-means, but some of the reasons the LLM provides for its answers are unclear (like the entirety of point 6), don't seem enough to justify the conclusion it reaches (like in point 5) or are again getting proposed due to the model following every technical terms it could find instead of the terms useful for our purpose (like in point 3).\n",
    "\n",
    "## Results: MiniBatch K-means - 1000 clusters\n",
    "Based on the full analysis of all representative reviews in the file, here are the most common themes where authors can improve their scientific papers:\n",
    "Key Areas for Improvement\n",
    "\n",
    "    Clarity and Impact of Results (\"result\" – 201 mentions)\n",
    "\n",
    "        Reviewers frequently focus on how clearly results are presented and whether they convincingly support the claims.\n",
    "\n",
    "    Quality and Design of Experiments (\"experiments\" – 142 mentions)\n",
    "\n",
    "        Thorough, well-designed experiments are essential. Reviewers note when experimental design is insufficient or poorly explained.\n",
    "\n",
    "    Comparative Analysis (\"comparison\", \"baseline\" – 121 mentions combined)\n",
    "\n",
    "        Papers should compare their methods against strong, relevant baselines and clearly show improvement.\n",
    "\n",
    "    Completeness (\"missing\", \"lack\" – 78 mentions combined)\n",
    "\n",
    "        Missing components like explanations, evaluations, or discussions weaken a paper’s credibility.\n",
    "\n",
    "    Novelty (\"novelty\" – 32 mentions)\n",
    "\n",
    "        A common concern is whether the contribution is original enough. Highlighting unique angles is critical.\n",
    "\n",
    "    Ablation Studies (\"ablation\" – 29 mentions)\n",
    "\n",
    "        Including these analyses to isolate the contribution of each component is valued.\n",
    "\n",
    "    Motivation and Significance (\"motivation\", \"significance\" – 45 mentions)\n",
    "\n",
    "        Clear motivation and demonstrated importance of the problem are often underemphasized.\n",
    "\n",
    "    Explanations and Methodology (\"explanation\", \"methodology\" – 23 mentions combined)\n",
    "\n",
    "        Reviewers dislike vague methods or unexplained decisions.\n",
    "\n",
    "    Unclear Wording or Structure (\"unclear\" – 17 mentions)\n",
    "\n",
    "        Clarity of writing and argumentation is crucial.\n",
    "\n",
    "Recommendations for Authors\n",
    "\n",
    "    Clearly communicate results, with visual aids and context that highlight their importance.\n",
    "\n",
    "    Design comprehensive experiments and explain every step and choice made.\n",
    "\n",
    "    Benchmark against relevant baselines and justify why those were chosen.\n",
    "\n",
    "    Avoid leaving gaps—ensure all expected sections and analyses are present.\n",
    "\n",
    "    Emphasize novelty with a clear comparison to prior work.\n",
    "\n",
    "    Include ablation studies when proposing complex models or methods.\n",
    "\n",
    "    Explain the \"why\" behind your work—motivate your approach in the intro and reinforce its impact in the conclusion.\n",
    "\n",
    "    Write clearly and precisely to minimize misinterpretation or confusion.\n",
    "***\n",
    "\n",
    "What the LLM did in this step was basically the frequency analysis of the given results based on a set of keywords it autonomously decide were the most important (looking at the code it generated, these are: 'novelty', 'unclear', 'missing', 'lack', 'ablation', 'baseline', 'motivation', 'experiments', 'comparison', 'result', 'significance', 'evaluation', 'explanation', 'methodology'). However, this approach is the same as the n-grams one we proposed at the start, which is way less computationally intensive than the proposed approach, hence these results are not satisfactory enough to justify the used approach. Also, the list of keywords the LLM uses is too restrictive to find interesting patterns. <br>\n",
    "To try and see whether this could be improved, we tried the same process, adding explicitly that the LLM **must not** use any keyword. The following are the obtained results:\n",
    "\n",
    "***\n",
    "Analyzing the full content of all 1,000 representative review excerpts—without relying on keywords—reveals several recurring reviewer sentiments and phrases that provide insights into how authors can write better papers. Here are some key findings:\n",
    "Common Reviewer Comments & Their Implications\n",
    "\n",
    "    “Vote reject paper.”\n",
    "\n",
    "        Direct rejection appears more than once, indicating recurring dissatisfaction. Authors should strive to address the root causes of rejection, often related to clarity, novelty, or rigor.\n",
    "\n",
    "    “Please clarify.” / “Please provide details.”\n",
    "\n",
    "        These reflect frequent requests for clearer explanations or elaboration, showing that vagueness or ambiguity in writing is a significant issue.\n",
    "\n",
    "    “Given clarifications author response, would willing increase score.”\n",
    "\n",
    "        Indicates that some reviewers are open to changing their opinion if confusions are resolved—implying that clarity and detailed communication can salvage reviews.\n",
    "\n",
    "    “However find number shortcomings need addressed.”\n",
    "\n",
    "        A typical critique suggesting that multiple flaws (in method, logic, or presentation) are common and must be minimized before submission.\n",
    "\n",
    "    “Additional feedback aim improve paper.” / “Provide additional feedback aim improve paper.”\n",
    "\n",
    "        Reviewers are giving constructive feedback, but authors must be receptive and proactive in addressing such suggestions.\n",
    "\n",
    "    Mentions of publication venues (e.g., “NeurIPS 2018”, “ICML 2019”)\n",
    "\n",
    "        Some representative texts reference other papers, possibly for comparison or benchmarking. Authors should situate their work clearly within the context of prior high-impact papers.\n",
    "\n",
    "    “Includes determining results, whether theoretical empirical, correct scientifically rigorous.”\n",
    "\n",
    "        Highlights the importance of sound methodology and solid results, both theoretical and empirical.\n",
    "\n",
    "    “Leaning towards rejecting paper.”\n",
    "\n",
    "        Shows how borderline papers can often go either way depending on presentation and completeness.\n",
    "\n",
    "Summary of Insights\n",
    "\n",
    "Authors can improve their chances of acceptance by:\n",
    "\n",
    "    Writing clearly and avoiding ambiguity, especially in explaining methods and results.\n",
    "\n",
    "    Anticipating reviewer doubts and addressing them proactively in the paper.\n",
    "\n",
    "    Being rigorous and comprehensive—reviewers reject when multiple shortcomings are found.\n",
    "\n",
    "    Understanding the expectations of top venues and aligning their work accordingly.\n",
    "\n",
    "    Taking reviewer feedback seriously, especially when it’s constructive or indicates potential for acceptance with revisions.\n",
    "***\n",
    "\n",
    "In this case, there are some results that are not relevant to our objective (“Given clarifications author response, would willing increase score.” is the perfect example of this, this is not something that can make authors write better papers) and, in general, these results almost seem like the LLM did not even analyze all the reviews or got stuck on some useless data it thought to be of interest.\n",
    "\n",
    "## Results analysis\n",
    "As it has been shown while looking at the above answers, the LLM seemingly performed worse on the given dataset if compared to the n-gram approach. This is probably due to the BERTopic model being swayed by the many technical words inside the reviews, even when these were divided into sentences to try and make this context get lost. We could try to clean the dataset even further by eliminating the papers' topics from the reviews, but this could prove detrimental for the extraction of critiques from it, which would completely defeat the purpose of our activity. An ulterior step we could try to do would be to practice sentimental analysis on the single sentences in order to try and find those that are more critical and more positive and try to extract data from them, but we don't think this approach would work, due to many of these possibly being nested in more mixed sentences. Because of this, these sentences could end up being lost due to not influencing that much the overall sentiment analysis score of the entire sentence. Thus, we have reasons to believe that a topic based approach is not suited for our purpose, even though in theory it could have worked. <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
